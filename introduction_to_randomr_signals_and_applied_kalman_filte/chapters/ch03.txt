The central problem of linear systems analysis is: Given the input, what is the
output?

In the deterministic case, we usually seek an explicit expression for the
response or output.

In the random-input problem no such explicit expression is
possible, except for the special case where the input is a so-called deterministic
random process (and not always in this case).

In the case of random processes the most
convenient descriptors to work with are autocorrelation function, power spectral
density function, and mean-square value.

3.1
In any system satisfying a set of linear differential equations, the solution may be
written as a superposition of an initial-condition part and another part due to the
driving or forcing functions.

Both the initial conditions and forcing functions may
be random; and, if so, the resultant response is a random process

Of course, in the stochastic problem, the input
and output will have to be described in probabilistic terms

Stationary (steady-state) analysis. Here the input is assumed to be time
stationary, and the system is assumed to have fixed parameters with a stable
transfer function
    This leads to a stationary output, provided the input has
been present for a long period of time relative to the system time constants.

Nonstationary (transient) analysis. Here we usually consider the driving
function as being applied at t 1⁄4 0, and the system may be initially at rest or
have nontrivial initial conditions.
    The response in this case is usually
nonstationary. We note that analysis of unstable systems falls into this
category, because no steady-state (stationary) condition will exist

3.2
We assume in Fig. 3.1 that G(s) represents a stable, fixed-parameter system and that
the input is covariance (wide-sense) stationary with a known power spectral density
function (PSD)

In deterministic analysis, we know that if the input is Fourier
transformable, the input spectrum is simply modified by G( jv) in going through the
filter.

In the random process case, one interpretation of the spectral function is that it
is proportional to the magnitude of the square of the Fourier transform

Thus, the
equation relating the input and output spectral functions is ...

Note that Eq. (3.2.1) is written in the s domain where the imaginary axis has the
meaning of real angular frequency v.

Because of the special properties of spectral functions, both sides of Eq. (3.2.2)
work out to be real functions of v

Also note that the autocorrelation function of the
output can be obtained as the inverse Fourier transform of S x ( jv).

The first term has all its poles and zeros in the left half-plane, and the second term
has mirror-image poles and zeros in the right half-plane. This regrouping of terms
is known as spectral factorization and can always be done if the spectral function
is rational in form (i.e., if it can be written as a ratio of polynomials in even powers
of s).

We note, however, that the concept of
power spectral density presented in Section 2.7 is perfectly general, and its integral
represents a mean-square value irrespective of whether or not the integral can be
evaluated in closed form.

3.3
In linear analysis problems, the spectral function can often be written as a ratio of
polynomials in s 2

If this is the case, spectral factorization can be used to write the
function in the form ...

    where c(s)/d(s) has all its poles and zeros in the left half-plane and c(–s)/d(–s) has
mirror-image poles and zeros in the right half-plane

No roots of d(s)
are permitted on the imaginary axis.

The mean-square value of x can now be
written as ...

3.4
We are now in a position to demonstrate the validity of using the pure white noise
model in certain problems, even though white noise has infinite variance

Consider a simple first-order low-pass filter with bandlimited white noise as
the input

Consider the same low-pass filter as in problem 1, but with pure white noise
as the input:

Certainly, problem 1 is physically plausible because bandlimited white
noise has finite variance. Conversely, problem 2 is not because the input has
infinite variance.

Now, we see by comparing the results given by Eqs. (3.4.6) and (3.4.8) that the
difference is just that between tan 1 (v c T ) and tan 1 (1)

The bandwidth of the
input is v c and the filter bandwidth is 1/T. Thus, if their ratio is large, tan 1 (v c T )
 tan 1 (1)

For a ratio of 100:1, the error is less than 1 percent. Thus, if the input
spectrum is flat considerably out beyond the point where the system response is
decreasing at 20 db/decade (or faster), there is relatively little error introduced by
assuming that the input is flat out to infinity

3.5
In filter theory, it is sometimes convenient to think of an idealized filter whose
frequency response is unity over a prescribed bandwidth B (in hertz) and zero
outside this band

If this ideal filter is driven
by white noise with amplitude A, its mean-square response is ...

Next, consider an actual filter G(s) whose gain has been normalized to yield a
peak response of unity

The mean-square
response of the actual filter to white noise of amplitude A is given by ...

Now, if we wish to find the idealized filter that will yield this same response, we
simply equate E(x 2 ) (ideal) and E(x 2 ) (actual) and solve for the bandwidth that gives
equality. The resultant bandwidth B is known as the noise equivalent bandwidth

3.6
If G(s) is minimum phase and rational in form, * Eq. (3.6.1) immediately provides a
factored form for S x (s) with poles and zeros automatically separated into left and
right half-plane parts.

What
minimum-phase transfer function will shape unity white noise into a given spectral
function S x (s)?

If we can use spectral factorization
on S x (s), the part with poles and zeros in the left half-plane provides the appropriate
shaping filter.

This is a useful concept, both as a mathematical artifice and also as a
physical means of obtaining a noise source with desired spectral characteristics
from a wideband source.

3.7
The block diagram of Fig. 3.1 is repeated as Fig. 3.5 with the addition of a
switch in the input.

Imagine the system to be initially at rest, and then close the
switch at t 1⁄4 0. A transient response takes place in the stochastic problem just as in
the corresponding deterministic problem.

If the input f(t) is a nondeterministic
random process, we would expect the response also to be nondeterministic, and its
autocorrelation function may be computed in terms of the input autocorrelation
function.

The system response can be written as a convolution integral ..
    where g(u) is the inverse Laplace transform of G(s) and is usually referred to as the
system weighting function or impulsive response.

find the autocorrelation function,

we now have an expression for the output autocorrelation function in terms of
the input autocorrelation function and system weighting function.

Equation (3.7.3) is difficult to evaluate except for relatively simple systems.
Thus, we are often willing to settle for less information about the response and just
compute its mean-square value.

Note that as t ! 1, the mean-square value approaches A/2T, which is the same
result obtained in Section 3.2 using spectral analysis methods.

Let G(s) be an integrator with zero initial conditions, and let f(t) be a Gauss–Markov
process with variance s 2 and time constant 1/b. We desire the mean-square value of
the output x.

The transfer function and input autocorrelation function are ...

Some care is required in evaluating Eq. (3.7.6) because one functional expression for
e bjuyj appliesforu > y,andadifferentoneappliesforu < y.

As our final example, we find the autocorrelation function of the output of a simple
integrator driven by unity-amplitude Gaussian white noise. The transfer function
and input autocorrelation function are ...

The
argument of the Dirac delta function in Eq. (3.7.9) is zero along the dashed line in the
figure

In concluding this section, we might comment that if the transient response
includes both forced and initial-condition components, the total response is just the
superposition of the two.

The mean-square value must be evaluated with care,
though, because the total mean-square value is the sum of the two only when the
crosscorrelation is zero

If the crosscorrelation between the two responses is not
zero, it must be properly accounted for in computing the mean-square value.

3.8
First of all, in the
term power spectral density (PSD) we will always interpret “power” to be mean-
square-value in squared units of the random variable under consideration.

Also, in
our analysis thus far we have dealt with frequency in terms of v rather than f, so
there has been a tacit assumption that the units of PSD in the denominator are
radians per second (rad/s). This is where the “units” problem begins.

The S x ( jv) was rigorously derived as the Fourier transform of the autocorrelation
function, and we are tempted to refer to S x ( jv) as density in units of meters 2 per rad/
s.

But yet the direct integral of S x ( jv) over the whole v space does not yield total
power. This is in conflict with the basic notion of the term “density”

Now, all this being said, we can actually perform the mechanics of the summation
with respect to v if we so choose; but, if we do so, we must be careful to put the 1/2p
factor in front of the integral

On the other hand, if we choose to integrate the same
S x (jv) as before but with respect to f, we then omit the 1/2p in front of the integral.

Pure white noise is a conceptual idea. It has a spectral density that is flat out to
infinity and its power is infinite (whatever that means).

Yet the PSD of white noise is
finite and has meaning, so we need to consider its units.

Eq. (3.8.2) says that the variance of the integrated white noise increases linearly
with time, and the scale factor of the ramp is just the PSD of the input white noise in
units (m/s) 2 /Hz


Unity white noise is a special white noise whose PSD is unity. It is dimen-
sionless in the numerator and has units of Hz in the denominator.

It is especially
useful as the forcing function in a conceptual shaping filter as discuss in Section 3.6.
When used in this context, the units for the output of the shaping filter are provided
by the filter transfer function, and not by the input unity white noise.

3.9
Even though the filter measurement stream may consist of discrete-time samples,
some of the underlying random processes may be time-continuous (e.g., physical
dynamics, Newton’s law, etc.). Thus, the continuous-time model is often just as
important as the corresponding discrete-time model.

Eq. (3.9.1) is a linear differential equation, and the components of x describe the
dynamics of the various processes under consideration.

Some of these process
descriptions may initially be in the form of power spectral densities, so we need to
be able to translate these PSD descriptions into differential equation form

Suppose we start with a PSD in rational form where both the numerator and
denominator are polynomials in v 2 (or s 2 ).

There are some restrictions on the orders
of the polynomials. These are imposed to assure that the variances of the phase
variables of the state vector will be finite.

The shaping filter and pole locations for this exercise are
shown in Fig. 3.10.

The scalar differential equation relating x(t) to u(t) is now obtained directly from
the transfer function of the shaping filter

Discrete-time processes may arise in either of two ways. First, there may be appli-
cations where a sequence of random events take place naturally in discrete steps, and
there is no underlying continuous dynamics to consider

Or, the discrete process may
be the result of sampling a continuous-time process such has physical motion
governed by Newton’s laws. It is such sampled processes that need further discussion
here.

So, let us now consider a continuous-time random process of the form given by
Eq. (3.9.1), but the accompanying measurements come to us sampled in the form ...

We only have measurements at discrete times t k , t kþ1 , t kþ2 , . . . , so we will
be primarily interested in the solution of the differential equation,

In linear systems, this solution can always be written
as the sum of an initial condition part and a driven-response part

Clearly, f ð t kþ1 ; t k Þ is the state transition matrix, and w(t k ) is the driven response
for the (t kþ1 , t k ) interval

Note that the scale factor for the white noise driving
function is accounted for in the G matrix

Also, we are assured that the w(t k ),
w(t kþ1 ), . . . sequence is a zero-mean white sequence, because we have assumed
that the driving function is vector unity white noise

The solution as given by
Eq. (3.9.8) is quite formidable if the system is time varying in the intervals
between samples. So, we will assume here that the Dt intervals are small, and that
the system parameters are constant within the Dt intervals.

With the constant parameter assumption in place, the state transition matrix can
be written out explicitly in terms of Laplace transforms ...

It will be shown in Chapter 4 that the covariance of w(k) is one of the key
parameters of the Kalman filter, so we need to be able to compute it as well as the
state transition matrix.

We will call the covariance of w(k), Q(t k ) (or sometimes, just
Q k for short), and it can be written out explicitly as ...

The matrix E[u(j)u (h)] is a matrix of Dirac delta functions that, presumably, is
known from the continuous model.

Thus, in principle, Q k may be evaluated from
Eq. (3.9.10). This is not a trivial task, though, even for low-order systems.

If the
continuous system giving rise to the discrete situation has constant parameters and if
the various white noise inputs have zero crosscorrelation, some simplification is
possible and the weighting function methods of Section 3.7 may be applied.

Analytical methods for finding f k and Q k work quite well for constant parameter
systems with just a few elements in the state vector

However, the dimensionality
does not have to be very large before it becomes virtually impossible to work out
explicit expressions for f k and Q

A numerical method for determining f k and Q k
for large scale systems has been worked out by C. F. van Loan [7], and it is
especially convenient when using MATLAB

3.10
Monte Carlo simulation refers to system simulation using random sequences as
inputs.

Such methods are often helpful in understanding the behavior of stochastic
systems that are not amenable to analysis by usual direct mathematical me-
thods. This is especially true of nonlinear filtering problems

Briefly, these methods involve setting up a statistical experiment that
matches the physical problem of interest, then repeating the experiment over and
over with typical sequences of random numbers, and finally, analyzing the results
of the experiment statistically

The usual description of a stationary continuous-time random process is its power
spectral density (PSD) function or the corresponding autocorrelation function

It
was mentioned in Chapter 2 that the autocorrelation function provides a complete
statistical description of the process when it is Gaussian

This is important in Monte
Carlo simulation (even though somewhat restrictive), because Gaussian processes
have a firm theoretical foundation and this adds credibility to the resulting analysis

we will begin with a
state model of the form given by Eq. (3.9.8) (Here we will use the more compact
notation where the subscript k refers to the time t k .) ...

Presumably, f k is known, and w k is a Gaussian white sequence with known
covariance Q k

The problem is to generate an ensemble of random trials of x k (i.e.,
sample realizations of the process) for k 1⁄4 0, 1, 2, . . . , m.

Equation (3.10.1) is explicit. Thus, once methods are established for generating
w k for k 1⁄4 0, 1, 2, . . . , (m – 1) and setting the initial condition for x at k 1⁄4 0, then
programming the few lines of code needed to implement Eq. (3.10.1) is routine.

If f k is a constant, it is simply assigned a variable name and
given a numerical value

If f k is time-variable, it is
relatively easy to reevaluate the parameters with each step as the simulation pro-
ceeds in time

Generating the w k sequence is a bit more difficult, though, because Q k
is usually not diagonal.

The preceding 2 	 2 example is a special case of what is known as Cholesky
factorization, and it is easily generalized to higher-order cases

This procedure
factors a symmetric, positive definite matrix into upper- and lower-triangular parts,

It should also be clear that if the transformation C takes a vector of uncorre-
lated, unit-variance random variables into a corresponding set of correlated random
variables, then C 1 will do just the opposite

If we start with a set of random vari-
ables with covariance Q, then C 1 QC 1T will be the covariance of the transformed
set.

One limitation of the Cholesky factorization is that it cannot be used if the cova-
riance matrix is singular.

We can overcome this limitation by the use of another
factorization method called the singular value decomposition

Specifying an appropriate initial condition on x in the simulation can also be
troublesome, and each case has to be considered on its own merits

If the process
being simulated is nonstationary, there is no “typical” starting point. This will
depend on the definition of the process.

If the process being considered is stationary, one usually wants to generate an
ensemble of realizations that are stationary throughout the time span of the runs.
The initial condition on x must be chosen carefully to assure this.

There is one
special case where specification of the initial components of x is relatively easy. If
the process is stationary and the state variables are chosen to be phase variables, it
works out that the covariance matrix of the state variables is diagonal in the steady-
state condition (see Problem 3.18)

Thus, one simply chooses the components of x
as independent samples from an N(0, 1) population appropriately scaled in accor-
dance with the rms values of the process “position,” “velocity,” “acceleration,” and
so forth.

If the state variables are not phase variables, however, then they will be
correlated (in general), and this complicates matters considerably.

Sometimes, the
most expeditious way of circumventing the problem is to start the simulation with
zero initial conditions, then let the process run until the steady-state condition is
reached (or nearly so), and finally use just the latter portion of the realization for
“serious” analysis. This may not be an elegant solution to the initial-condition
problem, but it is effective.

3.11
In steady-state analysis the primary descriptors for the random
processes being considered are autocorrelation function and power spectral density

They are Fourier transform pairs, so both contain the same information about
the process at hand

The process magnitude description of primary interest in our analysis is mean-
square-value, also called power.






























