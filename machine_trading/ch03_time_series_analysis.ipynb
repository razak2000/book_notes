{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may already have seen some time‐series analysis techniques in action in my previous books (Chan, 2009 and 2013), as a way to test for stationarity or cointegration of price series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time‐series techniques are most useful in markets where fundamental information and intuition are either lacking or not particularly useful for short‐term predictions. Currencies and bitcoins fit this bill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest model in time‐series analysis is AR(1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is just a linear regression model that relates the price in one bar to the next\n",
    "$Y(t) - \\mu = \\phi(Y(t-1) - \\mu) + \\epsilon(t)$\n",
    "\n",
    "   where $Y(t)$ is the price at time $t$, $\\phi$ is the (auto)regression coefficient, and $\\epsilon$ is Gaussian noise with zero mean, sometimes called innovation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A time series is called weakly1 stationary if its mean and variance are constant in time, and AR(1) is weakly stationary if images (the proof is left as an exercise). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A weakly stationary time series is also mean reverting (Chan, 2013). If images, the time series will trend. If images, we have a random walk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate images, we use the arima and estimate functions in the Econometrics Toolbox."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function images reduces to an AR(1) model if we set images and images (We will discuss the more general version in the next section.) The estimate function just applies maximum likelihood estimation to find the parameters for the AR(1) model based on the input price series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we tested on midprices instead of trade prices to reduce bid–ask bounce, which tends to produce phantom mean‐reversion that cannot really be traded on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalizing slightly from AR(1), we can consider images, represented by\n",
    "3.2images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this is just a multiple regression model with the price at time t as the dependent (response) variable and past prices up to a lag of images bars as independent (predictor) variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But introducing images as an additional parameter means that we can find the optimal images that gives the best fit of the images model to our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in many statistical models, we will use the Bayesian information criterion (BIC) that is proportional to the negative log likelihood of the model but with an additional term that is proportional to images, which penalizes complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective is to minimize BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have decided on the best estimate of images, we can apply the estimate function to it to find the coefficients images:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the next bar prediction has been made, we can use it to generate trading signals: Simply buy when the predicted price is higher than the current price, and sell when it is lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small extension of the AR model to include images lagged noise terms will often reduce the number of lags necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called the ARMA(p, q) model, or an auto‐regressive moving average process, where the images lagged noise terms are described as a moving average:\n",
    "3.3images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each images and images, we save the log likelihood in images, and images in images, the latter because it is used as a penalty term when minimizing BIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " How do we identify the optimal images and images that minimizes BIC from the LOGL and PQ matrices? We have to turn them into one‐dimensional vectors, apply the images function, and then use the images function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA(p, d, q) stands for autoregressive integrated moving average. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If images is an ARIMA(p, 1, q) model, it implies that images is an ARMA(p, q), where "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equivalence of an ARIMA(p, 1, q) model on log prices to an ARIMA(p, 0, q) model on log returns should not be confused with the statement that an ARMA(p, q) = ARIMA(p, 0, q) model on log prices is equivalent to some ARMA(images model on log returns. The latter statement is false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ARMA model in images's can always be transformed into an ARMA model in images's. But an ARMA model for images cannot always be transformed into an ARMA model for images. This is because an ARMA model for images can only have images as independent variables, whereas an ARMA model for images can have both images (which is just the difference of two imagess) and images as independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple autoregressive model AR(p) in equation 3.2 can be easily generalized to m multivariate time series. This generalized model is called a vector autoregressive model, or VAR(p). All we need to do is to interpret the autoregressive coefficients images as images matrices, and allow the noises images, which are m‐vectors to have nonzero cross‐sectional correlations but zero serial correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that images is not correlated with images, for any images, but images could be correlated with "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the autogressive coefficient matrices relate the current price of every time series to the lagged prices of all time series, VAR model is particularly suitable for modeling financial instruments that have correlated returns, such as a portfolio of stocks within the same industry group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To eliminate spurious mean‐reversion effects due to bid‐ask bounce, we will use midprices at market close provided by the Center for Research of Security Prices (CRSP) from January 3, 2007, to December 31, 2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the section on AR(p), we first need to determine the optimal lag p. We will use the first six years of data as training set for this determination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is decided, the other parameters of the model can be determined by the function vgxvarx, which is the equivalent of the estimate function for ARIMA models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions using this model on the out‐of‐sample data in 2013, use the vgxpred function, which is similar to the forecast function for ARIMA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In keeping with the linearity of the VAR models, we can construct a linear trading model as well. Furthermore, we can choose to make it sector‐neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the mean predicted return images of all the stocks in the industry group every day, and set the target dollar allocation of a stock to be proportional to the difference between its predicted return and the industry group mean,\n",
    "3.4images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often want to predict changes in price images instead of price images itself. So it is a bit awkward to use the VAR models, and the resulting AR coefficients do not make too much intuitive sense. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, VAR(p) can be transformed to a model with images as the dependent variable, and various lagged images's and images's as the independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called the VEC(q) (vector error correction) model, and is written as\n",
    "3.5images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images matrix C in equation 3.5 is called the error correction matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform the coefficients of VAR(p) to VEC(q), first note that images, and we can use the function vartovec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we do not need a cointegrating portfolio to use VEC(q) for prediction. Some of the stocks could be trending while others are mean reverting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AR, ARMA, VAR, and VEC models we have considered so far all use observable variables (prices of various lags) to predict their future values. However, econometricians have also concocted a class of models with hidden variables, called states, which can determine the values of observed variables (though subject to observation noise). These models are called state space models (SSM), a linear example of which is the Kalman filter, discussed in Chapter 3 of Chan (2013) and used in Chapter 5 in this book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A state space model starts with a linear relationship that specifies the time‐evolution of the hidden state variable, usually denoted by images:\n",
    "3.6images\n",
    "where images is an images‐dimensional vector, images and images are possibly time‐dependent but observable matrices (images is images, while images is images), and images is k‐dimensional Gaussian white noise with zero mean, unit variances, and zero serial and cross correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Equation 3.6 is often called the state transition equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observable variables (also called measurements) are related to the hidden variables by another linear equation\n",
    "3.7images\n",
    "where images is an images‐vector, images and images are possibly time‐dependent but observable matrices (images is images, while images is images), and images is images‐dimensional Gaussian white noise, also with zero mean, unit variances, and zero serial and cross correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Equation 3.7 is often called the measurement equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a hidden variable is the familiar moving average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can give some structure to this hidden variable images by requiring that it evolves in a particularly simple way:\n",
    "3.8images\n",
    "We have assumed images is the identity matrix, which is of course invariant in time, and images is an unknown but also time‐invariant matrix that determines the covariance of the estimation errors for the moving average images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we had said that images is supposed to be observable, it can be treated as an unknown parameter(s) to be estimated by applying maximum likelihood estimation on training data. (In other words, images is “observable” only to the extent that its values are not updated at each time step during Kalman filter updates.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the moving average (plural if the time series is multivariate) of a time series, a trader may hypothesize that the prices are trending, and thus the best guess for the observed price at time t is just the estimated moving average at time t as well:\n",
    "3.9images\n",
    "where images is another unknown and time‐invariant matrix to be estimated by MLE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that there are as many hidden state variables (five in total) as there are stocks in the computer hardware industry group. This is what a typical moving average model assumes as well—each price series has its own independent moving average. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we assume also that the state noise of one moving average is uncorrelated with any other but each may have a different variance. Hence, images is a images diagonal matrix with unknown parameters. (Unknown parameters are denoted as NaN as an input to the MATLAB estimate function.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we will assume the measurement noise of one stock's price is uncorrelated with another, but each may also have a different variance. Hence, images is also a images diagonal matrix with unkown parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One may also consider applying SSM on log prices instead, so that the Gaussian noise assumption is more reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the state transition and measurement equations are fixed, we can use the filter function to generate predictions of both the state and observation values.\n",
    "The images variable in the output of the filter function is the filtered price (moving average) at time images given observed prices up to time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model generates filtered prices that resemble the observed prices very closely, usually with less than 0.1 percent difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these predicted prices, we can calculate the predicted returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These predicted returns can be used in the same way as we did in the VAR model to create a sector‐neutral trading strategy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the moving average is not the only way the Kalman filter can be used to predict prices. If we assume trending behavior, we can also use it to find the slope of the recent trend in prices, leading to a prediction of the next price assuming the slope persists. This is left as an exercise for the reader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimates of the hidden state itself may be useful—after all, it is supposed to be a moving average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding estimates of a hidden variable in the presence of noise is the original meaning of filtering and is a well‐known concept in signal processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the Kalman filter, other well‐known filters in finance and economics include the Hodrick‐Prescott filter and the wavelet filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another application of Kalman filtering has been discussed in Chan (2013), where it was used to find the best estimates of the hedge ratio between two cointegrated price series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But instead of treating the two price series as measurements, we treat EWC as the measurements images, and EWA augmented with 1s as the time‐varying matrix images in equation 3.7. (The 1s are necessary to allow for the constant offset in the linear regression relationship between EWA and EWC.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the equity curve has started to flatten even during the latter part of the trainset. This could have been a result of regime change, where EWA and EWC have fallen out of cointegration, or more likely, a result of overfitting the noise covariance matrix images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
