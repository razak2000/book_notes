{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Physical systems are typically described\n",
    "in continuous time, but control and state estimation algorithms are typically im-\n",
    "plemented on digital computers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1.4 discusses some standard methods for\n",
    "obtaining a discrete-time representation of a continuous-time system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1.5\n",
    "discusses how to simulate continuous-time systems on a digital computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1\n",
    "A scalar is a single quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector consists of scalars that are arranged in a row or column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a scalar can be viewed as a 1-element vector; a scalar\n",
    "is a degenerate vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note\n",
    "that a vector can be viewed as a degenerate matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scalar can also be viewed as a degenerate matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank of a matrix is defined as the number of linearly independent rows. This\n",
    "is also equal to the number of linearly independent columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank of a matrix\n",
    "A is often indicated with the notation $\\sigma(A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank of a matrix is always less\n",
    "than or equal to the number of rows, and it is also less than or equal to the number\n",
    "of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix whose elements are com-\n",
    "prised entirely of zeros has a rank of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An $n \\times m$ matrix whose rank is equal\n",
    "to $\\min(n,m)$ is called full rank.\n",
    "te\n",
    "The nullity of an   $n \\times m$   matrix A is equal to\n",
    "[m - P ( 4 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A symmetric matrix is one for which $A = A^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hermitian transpose of a matrix (or vector) is the complex conjugate of the\n",
    "transpose, and is indicated with an H superscript, as in $A^H$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hermitian matrix is one for which A = $A^H$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we have a $p \\times n$ matrix H and an $n \\times n$ matrix P. Then $H^T$ is a\n",
    "n x p matrix, and we can compute the $p \\times p$ matrix product  $HPH^T$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix division is not defined; we cannot divide a matrix by another matrix\n",
    "(unless, of course, the denominator matrix is a scalar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The determinant of a matrix is defined inductively for square matrices. The\n",
    "determinant of a scalar (i.e., a 1 x 1 matrix) is equal to the scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The determinant of A is defined as\n",
    "for any value of i E [I, n]. This is called the Laplace expansion of A along its\n",
    "ith row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The determinant of\n",
    "A can also be defined as\n",
    "2=1\n",
    "(1.19)\n",
    "for any value of j E [l, n ] . This is called the Laplace expansion of A along its j t h\n",
    "column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix cannot have an inverse unless it is square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some square\n",
    "matrices do not have an inverse. A square matrix that does not have an inverse is\n",
    "called singular or invertible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the scalar case, the only number that does not have\n",
    "an inverse is the number 0. But in the matrix case, there are many matrices that\n",
    "are singular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix that does have an inverse is called nonsingular or invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nonsingularity of an $n \\times n$ matrix A can be stated in many equivalent\n",
    "ways\n",
    "    A is nonsingular.\n",
    "    0 A-l exists.\n",
    "    0 The rank of A is equal to n.\n",
    "    0 The rows of A are linearly independent.\n",
    "    0 The columns of A are linearly independent.\n",
    "    IAl # 0.\n",
    "    0 A z = b has a unique solution z for all b.\n",
    "    0 0 is not an eigenvalue of A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trace of a square matrix is defined as the sum of its diagonal elements:\n",
    "(1.24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trace of a matrix is defined only if the matrix is square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting property of the trace of a square matrix is\n",
    "a\n",
    "That is, the trace of a square matrix is equal to the sum of its eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trace of a matrix product is independent of the order in which\n",
    "the matrices are multiplied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two-norm of a column vector of real numbers, also called the Euclidean\n",
    "norm, is defined as follows:\n",
    "])x))2 =\n",
    "d z\n",
    "(1.27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An $n \\times n$ matrix A has n eigenvalues and n eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scalar X is an\n",
    "eigenvalue of A, and the n x 1 vector x is an eigenvector of A, if the following\n",
    "equation holds:\n",
    "AX = AX\n",
    "(1.30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvalues and eigenvectors of a matrix are collectively referred to as the\n",
    "eigend $A^TA$  of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An $n \\times n$ matrix has exactly n eigenvalues, although some may be repeated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if A has eigend $A^TA$  (X,z), then A2 has eigend $A^TA$  (X2,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown that\n",
    "A-l exists if and only if none of the eigenvalues of A are equal to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f A is\n",
    "symmetric then all of its eigenvalues are real numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A symmetric $n \\times n$ matrix A can be characterized as either positive definite,\n",
    "positive semidefinite, negative definite, negative semidefinite, or indefinite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive definite if xTAx > 0 for all nonzero n x 1 vectors z. This is equivalent\n",
    "to saying that all of the eigenvalues of A are positive real numbers. If A is\n",
    "positive definite, then A-' is also positive definite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive semidefinite if z T A z 2 0 for all n x 1 vectors z. This is equivalent to\n",
    "saying that all of the eigenvalues of A are nonnegative real numbers. Positive\n",
    "semidefinite matrices are sometimes called nonnegative definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative definite if z T A z < 0 for all nonzero n x 1 vectors z. This is equivalent\n",
    "t o saying that all of the eigenvalues of A are negative real numbers. If A is\n",
    "negative definite, then A-' is also negative definite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative semidefinite if z T A z 5 0 for all n x 1 vectors 2 . This is equivalent to\n",
    "saying that all of the eigenvalues of A are nonpositive real numbers. Negative\n",
    "semidefinite matrices are sometimes called nonpositive definite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indefinite if it does not fit into any of the above four categories. This is\n",
    "equivalent to saying that some of its eigenvalues are positive and some are\n",
    "negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighted two-norm of an n x 1 vector x is defined as\n",
    "; 1 . 1\n",
    "=\n",
    "mz\n",
    "(1.32)\n",
    "where Q is required to be an $n \\times n$ positive definite matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above norm is\n",
    "also called the Q-weighted two-norm of 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quantity of the form xTQz is called a\n",
    "quadratic in analogy to a quadratic term in a scalar equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The singular values g of a matrix A are defined as\n",
    "02(A)\n",
    "=\n",
    "X( $A^TA$ )\n",
    "= X(AA~)\n",
    "(1.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If A is an   $n \\times m$   matrix, then it has min(n,m) singular values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AT will have\n",
    "n eigenvalues, and  $A^TA$  will have m eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If n > m then $AA^T$ will have\n",
    "the same eigenvalues as  $A^TA$  plus an additional ( n - m) zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These additional\n",
    "zeros are not considered to be singular values of A , because A always has min(n, m)\n",
    "singular values. This knowledge can help reduce effort during the computation of\n",
    "singular values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have the partitioned matrix\n",
    "[ : E ]\n",
    "where A and D are invertible\n",
    "square matrices, and the B and C matrices may or may not be square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define\n",
    "E and F matrices as follows:\n",
    "E\n",
    "F\n",
    "D-CA-lB\n",
    "= A-BD-lC\n",
    "=\n",
    "(1.34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the definition of F to obtain\n",
    "( A - B D - l C ) - l = A-'\n",
    "+ A-lB(D - CA-'B)-lCA-l\n",
    "(1.38)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called the matrix inversion lemma. It is also referred to by other terms, such\n",
    "as the Sherman-Morrison formula, Woodbury's identity, and the modified matrices\n",
    "formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix inversion\n",
    "lemma is often stated in slightly different but equivalent ways. For example,\n",
    "( A + B D - l C ) - ' = A-' - A-'B(D\n",
    "+ CA-lB)-lCA-l\n",
    "(1.39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix inversion lemma can sometimes be used to reduce the computational\n",
    "effort of matrix inversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, with larger\n",
    "matrices, such as 1000 x 1000 matrices, the computational savings that is\n",
    "realized by using the matrix inversion lemma could be significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that A , B, C, and D are matrices, with A and D being square.\n",
    "[ 4 A I - l\n",
    "I 0 ] [ C\n",
    "A D\n",
    "B ] [ 0 I - A I - I B ] = [ A\n",
    "0 D - CA-lB\n",
    "]\n",
    "(1.46)\n",
    "(1.47)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, it can be shown that\n",
    "(1.48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These formulas are called product rules for determinants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix calculus\n",
    "As intuition would lead us to believe, the time derivative of a matrix is simply\n",
    "equal to the matrix of the time derivatives of the individual matrix elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also,\n",
    "the integral of a matrix is equal to the matrix of the integrals of the individual\n",
    "matrix elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that\n",
    "matrix A(t), which we will denote as A, has elements that are functions of time.\n",
    "We know that $AA^-1$ = I; that is, $AA^-1$ 6s a constant matrix and therefore has a\n",
    "time derivative of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the time derivative of $AA^-1$ can be computed as\n",
    "d\n",
    "dt\n",
    "-(AA-l)\n",
    "d\n",
    "= A A - l + A-((A-')\n",
    "dt\n",
    "(1.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is zero, we can solve for d(A-')/dt as\n",
    "- d ( ~ - 1 )\n",
    "= -A-~AA-~\n",
    "dt\n",
    "(1.51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that x is an n x 1 vector and f (x) is a scalar function of the elements\n",
    "of 2. Then\n",
    "af = [ af/axl . . . af/axn ]\n",
    "(1.53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though x is a column vector, d f / d x is a row vector. The converse is also\n",
    "true - if x is a row vector, then d f / d x is a column vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some authors\n",
    "define this the other way around. That is, they say that if x is a column vector then\n",
    "d f / d z is also a column vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no accepted convention for the definition of\n",
    "the partial derivative of a scalar with respect to a vector. It does not really matter\n",
    "which definition we use as long as we are consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that A is an $m \\times n$ matrix and f ( A ) is a scalar. Then the partial\n",
    "derivative of a scalar with respect to a matrix can be computed as follows:\n",
    "(1.54)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these definitions we can compute the partial derivative of the dot product of\n",
    "two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose x and y are n-element column vectors. Then\n",
    "xTy = x l y l + . . .\n",
    "+ znyn\n",
    "- -\n",
    "(1.55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we can obtain\n",
    "(1.56)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compute the partial derivative of a quadratic with respect to a vector.\n",
    "X ~ A X\n",
    "=\n",
    "[\n",
    "21\n",
    "' * *\n",
    "xn\n",
    "]\n",
    "[\n",
    "An1\n",
    ": I\n",
    "*\n",
    "*\n",
    "a\n",
    "Ann\n",
    "[\n",
    "X x n l\n",
    "]\n",
    "(1.57)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take the partial derivative of the quadratic as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If A is symmetric, as it often is in quadratic expressions, then A = AT and the\n",
    "above expression simplifies to\n",
    "d(xTAx)\n",
    "= 2xTA\n",
    "ax\n",
    "[ ''r) ]\n",
    "[ :'I.\n",
    "ifA=AT\n",
    "(1.59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose g ( z ) =\n",
    "gm ( x )\n",
    "andx=\n",
    "Then\n",
    "X n\n",
    "(1.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If either g(x) or x is transposed, then the partial derivative is also transposed.\n",
    "(1.61)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these definitions, the following important equalities can be derived. Suppose\n",
    "A is an $m \\times n$ matrix and x is an n x 1 vector. Then\n",
    "-\n",
    "a(Ax)\n",
    "- - A\n",
    "ax\n",
    "d(xTA)\n",
    "ax\n",
    "= A\n",
    "(1.62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we suppose that A is an $m \\times n$ matrix, B is an $n \\times n$ matrix, and we want\n",
    "to compute the partial derivative of Tr(ABAT) with respect to A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First compute\n",
    "ABA~\n",
    "as follows:\n",
    "(1.64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f B is symmetric, as it often is in partial derivatives of the form above, then this\n",
    "can be simplified to\n",
    "~ T ~ ( A B A ~ )\n",
    "=2AB\n",
    "dA\n",
    "ifB=BT\n",
    "(1.66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cauchy also discovered matrix\n",
    "eigenvalues and diagonalization, and introduced the idea of similar matrices. He\n",
    "was the first to prove that every real symmetric matrix is diagonalizable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that a\n",
    "matrix satisfies its own characteristic equation is now called the Cayley-Hamilton\n",
    "theorem (see Problem 1.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leon Mirskyâ€™s\n",
    "book in 1955 [MirSO] helped solidify matrix theory as a fundamentally important\n",
    "topic in university mathematics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 LINEAR SYSTEMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
