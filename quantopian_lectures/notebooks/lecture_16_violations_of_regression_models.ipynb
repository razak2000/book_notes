{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using a <font color='blue'>regression to fit a model to our data, the assumptions of regression analysis </font>must be satisfied in order to <font color='blue'>ensure good parameter estimates and accurate fit statistics. \n",
    "\n",
    "<font color='blue'>We would like parameters to be:\n",
    "\n",
    "- <font color='blue'>unbiased (expected value over different samples is the true value)\n",
    "- <font color='blue'>consistent (converging to the true value with many samples), and\n",
    "- <font color='blue'>efficient (minimized variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than focusing on your model construction, <font color='blue'>it is possible to gain a huge amount of information from your residuals (errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your model may be incredibly complex and impossible to analyze, but <font color='blue'>as long as you have predictions and observed values, you can compute residuals. Once you have your residuals you can perform many statistical tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>If your residuals do not follow a given distribution (usually normal, but depends on your model), then you know that something is wrong and you should be concerned with the accuracy of your predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>If the error term is not normally distributed, then our tests of statistical significance will be off. \n",
    "\n",
    "<font color='blue'>Fortunately, the central limit theorem tells us that, for large enough data samples, the coefficient distributions will be close to normal even if the errors are not. Therefore our analysis will still be valid for large data datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>A good test for normality is the Jarque-Bera test. It has a python implementation at `statsmodels.stats.stattools.jarque_bera` , we will use it frequently in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels import regression, stats\n",
    "import statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = np.random.poisson(size = 100)\n",
    "_, pvalue, _, _ = statsmodels.stats.stattools.jarque_bera(residuals)\n",
    "print pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Heteroskedasticity means that the variance of the error terms is not constant across observations. \n",
    "\n",
    "Intuitively, this means that the <font color='blue'>observations are not uniformly distributed along the regression line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It often occurs in <font color='blue'>cross-sectional data where the differences in the samples we are measuring lead to differences in the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results of linear regression\n",
    "slr1 = regression.linear_model.OLS(y1, sm.add_constant(xs)).fit()\n",
    "# Construct the fit line\n",
    "fit1 = slr1.params[0] + slr1.params[1]*xs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>You can test for heteroskedasticity using a few tests, we'll use the Breush Pagan test from the statsmodels library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also test for normality, which in this case also picks up the weirdness in the second case. HOWEVER, it is possible to have normally distributed residuals which are also heteroskedastic, so both tests must be performed to be sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_with_constant = sm.add_constant(xs)\n",
    "_, pvalue1, _, _ = stats.diagnostic.het_breushpagan(residuals1, xs_with_constant)\n",
    "_, pvalue2, _, _ = stats.diagnostic.het_breushpagan(residuals2, xs_with_constant)\n",
    "print \"p-value for residuals1 being heteroskedastic\", pvalue1\n",
    "print \"p-value for residuals2 being heteroskedastic\", pvalue2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>The problematic situation, known as conditional heteroskedasticity, is when the error variance is correlated with the independent variables as it is above. \n",
    "    \n",
    "This makes the F-test for regression significance and t-tests for the significances of individual coefficients unreliable. Most often this results in overestimation of the significance of the fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>The Breusch-Pagan test and the White test can be used to detect conditional heteroskedasticity. \n",
    "\n",
    "<font color='blue'>If we suspect that this effect is present, we can alter our model to try and correct for it. \n",
    "\n",
    "<font color='blue'>One method is generalized least squares, which requires a manual alteration of the original equation. \n",
    "\n",
    "<font color='blue'>Another is computing robust standard errors, which corrects the fit statistics to account for the heteroskedasticity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print slr2.summary()\n",
    "print slr2.get_robustcov_results().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>A common and serious problem is when errors are correlated across observations (known serial correlation or autocorrelation). </font>This can occur, for instance, <font color='blue'>when some of the data points are related, or when we use time-series data with periodic fluctuations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>If one of the independent variables depends on previous values of the dependent variable - such as when it is equal to the value of the dependent variable in the previous period - or if incorrect model specification leads to autocorrelation, then the coefficient estimates will be inconsistent and therefore invalid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Otherwise, the parameter estimates will be valid, but the fit statistics will be off.</font> For instance, if the correlation is positive, we will have inflated F- and t-statistics, leading us to overestimate the significance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>If the errors are homoskedastic, we can test for autocorrelation using the Durbin-Watson test, which is conveniently reported in the regression summary in `statsmodels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pricing data for an asset\n",
    "start = '2014-01-01'\n",
    "end = '2015-01-01'\n",
    "y = get_pricing('DAL', fields='price', start_date=start, end_date=end)\n",
    "x = np.arange(len(y))\n",
    "# Regress pricing data against time\n",
    "model = regression.linear_model.OLS(y, sm.add_constant(x)).fit()\n",
    "# Construct the fit line\n",
    "prediction = model.params[0] + model.params[1]*x\n",
    "# Plot pricing data and regression line\n",
    "plt.plot(x,y)\n",
    "plt.plot(x, prediction, color='r')\n",
    "plt.legend(['DAL Price', 'Regression Line'])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "# Print summary of regression results\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can <font color='blue'>test for autocorrelation</font> in both our prices and residuals. We'll use the built-in method to do this, which is based on the <font color='blue'>Ljun-Box test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>This test computes the probability that the n-th lagged datapoint is predictive of the current. If no max lag is given, then the function computes a max lag and returns the p-values for all lags up to that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, prices_qstats, prices_qstat_pvalues = statsmodels.tsa.stattools.acf(y, qstat=True)\n",
    "_, prices_qstats, prices_qstat_pvalues = statsmodels.tsa.stattools.acf(y-prediction, qstat=True)\n",
    "print 'Prices autocorrelation p-values', prices_qstat_pvalues\n",
    "print 'Residuals autocorrelation p-values', prices_qstat_pvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Newey-West is a method of computing variance which accounts for autocorrelation. A naive variance computation will actually produce inaccurate standard errors with the presence of autocorrelation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>We can attempt to change the regression equation to eliminate serial correlation. A simpler fix is adjusting the standard errors using an appropriate method and using the adjusted values to check for significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we use the Newey-West method from `statsmodels` to compute adjusted standard errors for the coefficients. They are higher than those originally reported by the regression, which is what we expected for positively correlated errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "# Find the covariance matrix of the coefficients\n",
    "cov_mat = stats.sandwich_covariance.cov_hac(model)\n",
    "# Print the standard errors of each coefficient from the original model and from the adjustment\n",
    "print 'Old standard errors:', model.bse[0], model.bse[1]\n",
    "print 'Adjusted standard errors:', sqrt(cov_mat[0,0]), sqrt(cov_mat[1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>When using multiple independent variables, it is important to check for multicollinearity; that is, an approximate linear relation between the independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>As with truly unnecessary variables, this will usually not hurt the accuracy of the model, but will cloud our analysis. In particular, the estimated coefficients will have large standard errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High correlation between independent variables is indicative of multicollinearity. <font color='blue'>However, it is not enough, since we would want to detect correlation between one of the variables and a linear combination of the other variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>If we have high R-squared but low t-statistics on the coefficients (the fit is good but the coefficients are not estimated precisely) we may suspect multicollinearity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>To resolve the problem, we can drop one of the independent variables involved in the linear relation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, using two stock indices as our independent variables is likely to lead to multicollinearity. Below we can see that removing one of them improves the t-statistics without hurting R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Another important thing to determine here is which variable may be the casual one. </font>If we hypothesize that the market influences both MDY and HPQ, then the market is the variable that we should use in our predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pricing data for asset and two market indices\n",
    "start = '2014-01-01'\n",
    "end = '2015-01-01'\n",
    "b1 = get_pricing('SPY', fields='price', start_date=start, end_date=end)\n",
    "b2 = get_pricing('MDY', fields='price', start_date=start, end_date=end)\n",
    "a = get_pricing('HPQ', fields='price', start_date=start, end_date=end)\n",
    "# Run multiple linear regression\n",
    "mlr = regression.linear_model.OLS(a, sm.add_constant(np.column_stack((b1,b2)))).fit()\n",
    "# Construct fit curve using dependent variables and estimated coefficients\n",
    "mlr_prediction = mlr.params[0] + mlr.params[1]*b1 + mlr.params[2]*b2\n",
    "# Print regression statistics \n",
    "print 'R-squared:', mlr.rsquared_adj\n",
    "print 't-statistics of coefficients:\\n', mlr.tvalues\n",
    "# Plot asset and model\n",
    "a.plot()\n",
    "mlr_prediction.plot()\n",
    "plt.legend(['Asset', 'Model']);\n",
    "plt.ylabel('Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slr = regression.linear_model.OLS(a, sm.add_constant(b1)).fit()\n",
    "slr_prediction = slr.params[0] + slr.params[1]*b1\n",
    "# Print fit statistics\n",
    "print 'R-squared:', slr.rsquared_adj\n",
    "print 't-statistics of coefficients:\\n', slr.tvalues\n",
    "# Plot asset and model\n",
    "a.plot()\n",
    "slr_prediction.plot()\n",
    "plt.ylabel('Price')\n",
    "plt.legend(['Asset', 'Model']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "# Construct Anscombe's arrays\n",
    "x1 = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5]\n",
    "y1 = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]\n",
    "x2 = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5]\n",
    "y2 = [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74]\n",
    "x3 = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5]\n",
    "y3 = [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73]\n",
    "x4 = [8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8]\n",
    "y4 = [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]\n",
    "\n",
    "# Perform linear regressions on the datasets\n",
    "slr1 = regression.linear_model.OLS(y1, sm.add_constant(x1)).fit()\n",
    "slr2 = regression.linear_model.OLS(y2, sm.add_constant(x2)).fit()\n",
    "slr3 = regression.linear_model.OLS(y3, sm.add_constant(x3)).fit()\n",
    "slr4 = regression.linear_model.OLS(y4, sm.add_constant(x4)).fit()\n",
    "\n",
    "# Print regression coefficients, Pearson r, and R-squared for the 4 datasets\n",
    "print 'Cofficients:', slr1.params, slr2.params, slr3.params, slr4.params\n",
    "print 'Pearson r:', pearsonr(x1, y1)[0], pearsonr(x2, y2)[0], pearsonr(x3, y3)[0], pearsonr(x4, y4)[0]\n",
    "print 'R-squared:', slr1.rsquared, slr2.rsquared, slr3.rsquared, slr4.rsquared\n",
    "\n",
    "# Plot the 4 datasets with their regression lines\n",
    "f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)\n",
    "xs = np.arange(20)\n",
    "ax1.plot(slr1.params[0] + slr1.params[1]*xs, 'r')\n",
    "ax1.scatter(x1, y1)\n",
    "ax1.set_xlabel('x1')\n",
    "ax1.set_ylabel('y1')\n",
    "ax2.plot(slr2.params[0] + slr2.params[1]*xs, 'r')\n",
    "ax2.scatter(x2, y2)\n",
    "ax2.set_xlabel('x2')\n",
    "ax2.set_ylabel('y2')\n",
    "ax3.plot(slr3.params[0] + slr3.params[1]*xs, 'r')\n",
    "ax3.scatter(x3, y3)\n",
    "ax3.set_xlabel('x3')\n",
    "ax3.set_ylabel('y3')\n",
    "ax4.plot(slr4.params[0] + slr4.params[1]*xs, 'r')\n",
    "ax4.scatter(x4,y4)\n",
    "ax4.set_xlabel('x4')\n",
    "ax4.set_ylabel('y4');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
