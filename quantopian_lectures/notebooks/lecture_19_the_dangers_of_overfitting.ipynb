{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>We say a model is overfit when it is overly sensitive to noise and idiosyncracies in the sample data, and therefore does not reflect the underlying data-generating process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <font color='blue'>two broad causes of overfitting are:\n",
    "- <font color='blue'>small sample size</font>, so that noise and trend are not distinguishable\n",
    "- <font color='blue'>choosing an overly complex model</font>, so that it ends up contorting to fit the noise in the sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Generalizing this to stocks, if your model starts developing many specific rules based on specific past events, it is almost definitely overfitting. \n",
    "\n",
    "This is why black-box machine learning (neural networks, etc.) is so dangerous when not done correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple linear models aren't suitable for all situations, especially when we have reason to believe that the data is nonlinear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels import regression\n",
    "from scipy import poly1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "y = 2*np.random.randn(10) + x**2\n",
    "xs = np.linspace(-0.25, 9.25, 200)\n",
    "\n",
    "lin = np.polyfit(x, y, 1)\n",
    "quad = np.polyfit(x, y, 2)\n",
    "many = np.polyfit(x, y, 9)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xs, poly1d(lin)(xs))\n",
    "plt.plot(xs, poly1d(quad)(xs))\n",
    "plt.plot(xs, poly1d(many)(xs))\n",
    "plt.ylabel('Y')\n",
    "plt.xlabel('X')\n",
    "plt.legend(['Underfit', 'Good fit', 'Overfit']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>When working with real data, our choice of function should reflect a belief about the underlying process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Just as the most elegant physics models describe a tremendous amount of our world through a few equations, a good trading model should explain most of the data through a few rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any time you start to have a number of rules even close to the number of points in your data set, you can be sure you are overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Since parameters can be thought of as rules as they equivalently constrain a model, the same is true of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Fewer parameters is better, and it is better to explain 60% of the data with 2-3 parameters than 90% with 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Because there is almost always noise present in real data, a perfect fit is almost always indicative of overfitting.</font> It is almost impossible to know the percentage noise/signal in a given data set while you are developing the model, but use your common sense. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know which variables to include in a model? If we're afraid of omitting something important, we might try different ones and include all the variables we can find that improve the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one year's worth of pricing data for five different assets\n",
    "start = '2013-01-01'\n",
    "end = '2014-01-01'\n",
    "x1 = get_pricing('PEP', fields='price', start_date=start, end_date=end)\n",
    "x2 = get_pricing('MCD', fields='price', start_date=start, end_date=end)\n",
    "x3 = get_pricing('ATHN', fields='price', start_date=start, end_date=end)\n",
    "x4 = get_pricing('DOW', fields='price', start_date=start, end_date=end)\n",
    "y = get_pricing('PG', fields='price', start_date=start, end_date=end)\n",
    "\n",
    "# Build a linear model using only x1 to explain y\n",
    "slr = regression.linear_model.OLS(y, sm.add_constant(x1)).fit()\n",
    "slr_prediction = slr.params[0] + slr.params[1]*x1\n",
    "\n",
    "# Run multiple linear regression using x1, x2, x3, x4 to explain y\n",
    "mlr = regression.linear_model.OLS(y, sm.add_constant(np.column_stack((x1,x2,x3,x4)))).fit()\n",
    "mlr_prediction = mlr.params[0] + mlr.params[1]*x1 + mlr.params[2]*x2 + mlr.params[3]*x3 + mlr.params[4]*x4\n",
    "\n",
    "# Compute adjusted R-squared for the two different models\n",
    "print 'SLR R-squared:', slr.rsquared_adj\n",
    "print 'SLR p-value:', slr.f_pvalue\n",
    "print 'MLR R-squared:', mlr.rsquared_adj\n",
    "print 'MLR p-value:', mlr.f_pvalue\n",
    "\n",
    "# Plot y along with the two different predictions\n",
    "y.plot()\n",
    "slr_prediction.plot()\n",
    "mlr_prediction.plot()\n",
    "plt.ylabel('Price')\n",
    "plt.xlabel('Date')\n",
    "plt.legend(['PG', 'SLR', 'MLR']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In our initial timeframe, <font color='blue'>we are able to fit the model more closely to the data when using multiple variables than when using just one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>However, when we use the same estimated parameters to model a different time period, we find that the single-variable model fits worse, while the multiple-variable model is entirely useless. </font>It seems that the relationships we found are not consistent and are particular to the original sample period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the next of pricing data\n",
    "start = '2014-01-01'\n",
    "end = '2015-01-01'\n",
    "x1 = get_pricing('PEP', fields='price', start_date=start, end_date=end)\n",
    "x2 = get_pricing('MCD', fields='price', start_date=start, end_date=end)\n",
    "x3 = get_pricing('ATHN', fields='price', start_date=start, end_date=end)\n",
    "x4 = get_pricing('DOW', fields='price', start_date=start, end_date=end)\n",
    "y = get_pricing('PG', fields='price', start_date=start, end_date=end)\n",
    "\n",
    "# Extend our model from before to the new time period\n",
    "slr_prediction2 = slr.params[0] + slr.params[1]*x1\n",
    "mlr_prediction2 = mlr.params[0] + mlr.params[1]*x1 + mlr.params[2]*x2 + mlr.params[3]*x3 + mlr.params[4]*x4\n",
    "\n",
    "# Manually compute adjusted R-squared over the new time period\n",
    "\n",
    "# Adjustment 1 is for the SLR model\n",
    "p = 1\n",
    "N = len(y)\n",
    "adj1 = float(N - 1)/(N - p - 1)\n",
    "\n",
    "# Now for MLR\n",
    "p = 4\n",
    "N = len(y)\n",
    "adj2 = float(N - 1)/(N - p - 1)\n",
    "\n",
    "SST = sum((y - np.mean(y))**2)\n",
    "SSRs = sum((slr_prediction2 - y)**2)\n",
    "print 'SLR R-squared:', 1 - adj1*SSRs/SST\n",
    "SSRm = sum((mlr_prediction2 - y)**2)\n",
    "print 'MLR R-squared:', 1 - adj2*SSRm/SST\n",
    "\n",
    "# Plot y along with the two different predictions\n",
    "y.plot()\n",
    "slr_prediction2.plot()\n",
    "mlr_prediction2.plot()\n",
    "plt.ylabel('Price')\n",
    "plt.xlabel('Date')\n",
    "plt.legend(['PG', 'SLR', 'MLR']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>One of the challenges in building a model that uses rolling parameter estimates, such as rolling mean or rolling beta, is choosing a window length. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>A longer window will take into account long-term trends and be less volatile, but it will also lag more when taking into account new observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>The choice of window length strongly affects the rolling parameter estimate and can change how we see and treat the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pricing data for a stock\n",
    "start = '2011-01-01'\n",
    "end = '2013-01-01'\n",
    "pricing = get_pricing('MCD', fields='price', start_date=start, end_date=end)\n",
    "\n",
    "# Compute rolling averages for various window lengths\n",
    "mu_30d = pricing.rolling(window=30).mean()\n",
    "mu_60d = pricing.rolling(window=60).mean()\n",
    "mu_100d = pricing.rolling(window=100).mean()\n",
    "\n",
    "# Plot asset pricing data with rolling means from the 100th day, when all the means become available\n",
    "plt.plot(pricing[100:], label='Asset')\n",
    "plt.plot(mu_30d[100:], label='30d MA')\n",
    "plt.plot(mu_60d[100:], label='60d MA')\n",
    "plt.plot(mu_100d[100:], label='100d MA')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Price')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>If we pick the length based on which seems best - say, on how well our model or algorithm performs - we are overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trade using a simple mean-reversion strategy\n",
    "def trade(stock, length):\n",
    "    \n",
    "    # If window length is 0, algorithm doesn't make sense, so exit\n",
    "    if length == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Compute rolling mean and rolling standard deviation\n",
    "    rolling_window = stock.rolling(window=length)\n",
    "    mu = rolling_window.mean()\n",
    "    std = rolling_window.std()\n",
    "    \n",
    "    # Compute the z-scores for each day using the historical data up to that day\n",
    "    zscores = (stock - mu)/std\n",
    "    \n",
    "    # Simulate trading\n",
    "    # Start with no money and no positions\n",
    "    money = 0\n",
    "    count = 0\n",
    "    for i in range(len(stock)):\n",
    "        # Sell short if the z-score is > 1\n",
    "        if zscores[i] > 1:\n",
    "            money += stock[i]\n",
    "            count -= 1\n",
    "        # Buy long if the z-score is < 1\n",
    "        elif zscores[i] < -1:\n",
    "            money -= stock[i]\n",
    "            count += 1\n",
    "        # Clear positions if the z-score between -.5 and .5\n",
    "        elif abs(zscores[i]) < 0.5:\n",
    "            money += count*stock[i]\n",
    "            count = 0\n",
    "    return money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the window length 0-254 that gives the highest returns using this strategy\n",
    "length_scores = [trade(pricing, l) for l in range(255)]\n",
    "best_length = np.argmax(length_scores)\n",
    "print 'Best window length:', best_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start2 = '2013-01-01'\n",
    "end2 = '2015-01-01'\n",
    "pricing2 = get_pricing('MCD', fields='price', start_date=start2, end_date=end2)\n",
    "\n",
    "# Find the returns during this period using what we think is the best window length\n",
    "length_scores2 = [trade(pricing2, l) for l in range(255)]\n",
    "print best_length, 'day window:', length_scores2[best_length]\n",
    "\n",
    "# Find the best window length based on this dataset, and the returns using this window length\n",
    "best_length2 = np.argmax(length_scores2)\n",
    "print best_length2, 'day window:', length_scores2[best_length2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(length_scores)\n",
    "plt.plot(length_scores2)\n",
    "plt.xlabel('Window length')\n",
    "plt.ylabel('Score')\n",
    "plt.legend(['2011-2013', '2013-2015']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>To avoid overfitting, we can use economic reasoning or the nature of our algorithm to pick our window length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>We can also use Kalman filters, which do not require us to specify a length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>We can try to avoid overfitting by taking large samples, choosing reasonable and simple models, and not cherry-picking parameters to fit the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure we haven't broken our model with overfitting, we have to test out of sample. <font color='blue'>If we cannot gather large amounts of additional data at will, we should split the sample we have into two parts, of which one is reserved for testing only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Cross validation</font> is the process of splitting your data into n parts, then estimating optimal parameters for n-1 parts combined and testing on the final part. By doing this n times, one for each part held out, we can establish how stable our parameter estimates are and how predictive they are on data not from the original set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Information criterion are a rigorous statistical way to test if the amount of complexity in your model is worth the extra predictive power.</font> The test favors simpler models and will tell you if you are introducing a large amount of complexity without much return. <font color='blue'>One of the most common methods is [Akaike Information Criterion.](https://en.wikipedia.org/wiki/Akaike_information_criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
