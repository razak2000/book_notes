{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple <font color='blue'>comparisons bias</font> is a pervasive problem in statistics, data science, and in general forecasting/predictions. The short explanation is that the more tests you run, <font color='blue'>the more likely you are to get an outcome that you want/expect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>A particularly common example of this is when looking for relationships in large data sets comprising of many indepedent series or variables. In this case you run a test each time you evaluate whether a relationship exists between a set of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Spearman Rank Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>It's a variation of correlation that takes into account the ranks of the data. This can help with weird distributions or outliers that would confuse other measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>A higher coefficient means a stronger estimated relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.Series(np.random.normal(0, 1, 100))\n",
    "Y = X\n",
    "\n",
    "r_s = stats.spearmanr(Y, X)\n",
    "print 'Spearman Rank Coefficient: ', r_s[0]\n",
    "print 'p-value: ', r_s[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.Series(np.random.normal(0, 1, 100))\n",
    "Y = X + np.random.normal(0, 1, 100)\n",
    "\n",
    "r_s = stats.spearmanr(Y, X)\n",
    "print 'Spearman Rank Coefficient: ', r_s[0]\n",
    "print 'p-value: ', r_s[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>p-values must be treated as binary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>A common mistake is that p-values are treated as more or less significant. This is bad practice as it allows for what's known as [p-hacking](https://en.wikipedia.org/wiki/Data_dredging) and will result in more false positives than you expect. Effectively, you will be too likely to convince yourself that relationships exist in your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To treat p-values as binary,<font color='blue'> a cutoff must be set in advance. Then the p-value must be compared with the cutoff and treated as significant/not signficant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can refer to the cutoff as our <font color='blue'>significance level because a lower cutoff means that results which pass it are significant at a higher level of confidence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if you have a cutoff of 0.05, then even on random data 5% of tests will pass based on chance. A cutoff of 0.01 reduces this to 1%, which is a more stringent test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a cutoff of 5% means that there is a 5% chance\n",
    "# of us getting a significant p-value given no relationship\n",
    "# in our data (false positive).\n",
    "# NOTE: This is only true if the test's assumptions have been\n",
    "# satisfied and the test is therefore properly calibrated.\n",
    "# All tests have different assumptions.\n",
    "cutoff = 0.05\n",
    "\n",
    "X = pd.Series(np.random.normal(0, 1, 100))\n",
    "Y = X + np.random.normal(0, 1, 100)\n",
    "\n",
    "r_s = stats.spearmanr(Y, X)\n",
    "print 'Spearman Rank Coefficient: ', r_s[0]\n",
    "if r_s[1] < cutoff:\n",
    "    print 'There is significant evidence of a relationship.'\n",
    "else:\n",
    "    print 'There is not significant evidence of a relationship.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "T = 100\n",
    "\n",
    "for i in range(N):\n",
    "    X = np.random.normal(0, 1, T)\n",
    "    X = pd.Series(X)\n",
    "    name = 'X%s' % i\n",
    "    df[name] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.05\n",
    "\n",
    "significant_pairs = []\n",
    "\n",
    "for i in range(N):\n",
    "    for j in range(i+1, N):\n",
    "        Xi = df.iloc[:, i]\n",
    "        Xj = df.iloc[:, j]\n",
    "        \n",
    "        results = stats.spearmanr(Xi, Xj)\n",
    "        \n",
    "        pvalue = results[1]\n",
    "        \n",
    "        if pvalue < cutoff:\n",
    "            significant_pairs.append((i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_experiment(N, T, cutoff=0.05):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Make random data\n",
    "    for i in range(N):\n",
    "        X = np.random.normal(0, 1, T)\n",
    "        X = pd.Series(X)\n",
    "        name = 'X%s' % i\n",
    "        df[name] = X\n",
    "\n",
    "    significant_pairs = []\n",
    "\n",
    "    # Look for relationships\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            Xi = df.iloc[:, i]\n",
    "            Xj = df.iloc[:, j]\n",
    "\n",
    "            results = stats.spearmanr(Xi, Xj)\n",
    "\n",
    "            pvalue = results[1]\n",
    "\n",
    "            if pvalue < cutoff:\n",
    "                significant_pairs.append((i, j))\n",
    "    \n",
    "    return significant_pairs\n",
    "\n",
    "\n",
    "num_experiments = 100\n",
    "\n",
    "results = np.zeros((num_experiments,))\n",
    "\n",
    "for i in range(num_experiments):\n",
    "    # Run a single experiment\n",
    "    result = do_experiment(20, 100, cutoff=0.05)\n",
    "    \n",
    "    # Count how many pairs\n",
    "    n = len(result)\n",
    "    \n",
    "    # Add to array\n",
    "    results[i] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>What's happening here is that p-values should be uniformly distributed, given no signal in the underlying data. \n",
    "\n",
    "Basically, they carry no information whatsoever and will be equally likely to be 0.01 as 0.99. \n",
    "\n",
    "<font color='blue'>Because they're popping out randomly, you will expect a certain percentage of p-values to be underneath any threshold you choose. The lower the threshold the fewer will pass your test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pvalues_from_experiment(N, T):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Make random data\n",
    "    for i in range(N):\n",
    "        X = np.random.normal(0, 1, T)\n",
    "        X = pd.Series(X)\n",
    "        name = 'X%s' % i\n",
    "        df[name] = X\n",
    "\n",
    "    pvalues = []\n",
    "\n",
    "    # Look for relationships\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            Xi = df.iloc[:, i]\n",
    "            Xj = df.iloc[:, j]\n",
    "\n",
    "            results = stats.spearmanr(Xi, Xj)\n",
    "\n",
    "            pvalue = results[1]\n",
    "\n",
    "            pvalues.append(pvalue)\n",
    "    \n",
    "    return pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalues = get_pvalues_from_experiment(10, 100)\n",
    "    plt.hist(pvalues)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Observed p-value');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalues = get_pvalues_from_experiment(50, 100)\n",
    "plt.hist(pvalues)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Observed p-value');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalues = get_pvalues_from_experiment(50, 100)\n",
    "plt.vlines(0.01, 0, 150, colors='r', linestyle='--', label='0.01 Cutoff')\n",
    "plt.vlines(0.05, 0, 150, colors='r', label='0.05 Cutoff')\n",
    "plt.hist(pvalues, label='P-Value Distribution')\n",
    "plt.legend()\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Observed p-value');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that <font color='blue'>with a lower cutoff we should expect to get fewer false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiments = 100\n",
    "\n",
    "results = np.zeros((num_experiments,))\n",
    "\n",
    "for i in range(num_experiments):\n",
    "    # Run a single experiment\n",
    "    result = do_experiment(20, 100, cutoff=0.01)\n",
    "    \n",
    "    # Count how many pairs\n",
    "    n = len(result)\n",
    "    \n",
    "    # Add to array\n",
    "    results[i] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with any adjustment of p-value cutoff, we have a tradeoff. <font color='blue'>A lower cutoff decreases the rate of false positives, but also decreases the chance we find a real relationship (true positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>So you can't just decrease your cutoff to solve this problem. https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>You can't really eliminate multiple comparisons bias, but you can reduce how much it impacts you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run fewer tests. <font color='blue'>Rather than just sweeping around hoping you hit an interesting signal, use your expert knowledge of the system to develop a great hypothesis and test that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process of exploring the data, coming up with a hypothesis, then gathering more data and testing the hypothesis on the new data is considered the gold standard in statistical and scientific research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>It's crucial that the data set on which you develop your hypothesis is not the one on which you test it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Moving to new data and testing there will not only mean you only run one test, but will be an 'unbiased estimator' of whether your hypothesis is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>If you must run many tests, try to correct your p-values. This means applying a correction factor to the cutoff you desire to obtain the one actually used when determining whether p-values are significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>The most conservative and common correction factor is Bon Ferroni."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept behind Bon Ferroni is quite simple. It just says that<font color='blue'> if we run  m  tests, and we have a significance level/cutoff of  a , then we should use  a/m  as our new cutoff when determining significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiments = 100\n",
    "results = np.zeros((num_experiments,))\n",
    "N = 20\n",
    "T = 100\n",
    "desired_level = 0.05\n",
    "num_tests = N * (N - 1) / 2\n",
    "new_cutoff = desired_level / num_tests\n",
    "for i in range(num_experiments):\n",
    "    # Run a single experiment\n",
    "    result = do_experiment(20, 100, cutoff=new_cutoff)\n",
    "    # Count how many pairs\n",
    "    n = len(result)\n",
    "    # Add to array\n",
    "    results[i] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Because Bon Ferroni is so stringent, you can often end up passing over real relationships. \n",
    "\n",
    "There is a good example in the following article https://en.wikipedia.org/wiki/Multiple_comparisons_problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effectively, <font color='blue'>it assumes that all the tests you are running are independent, and doesn't take into account any structure in your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be able to design a more finely tuned correction factor, but this is adding a layer of complexity and therefore a point of failure to your research. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Because of the over-zealousness of Bon Ferroni, often running fewer tests is the better option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>if you must run many tests, reserve multiple sets of data so your candidate signals can undergo an out-of-sample round of testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p-hacking is just intentional or accidental abuse of multiple comparisons bias. It is surprisingly common, even in academic literature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The excellent statistical news website FiveThirtyEight has a great visualization here: https://fivethirtyeight.com/features/science-isnt-broken/\n",
    "\n",
    "Wikipedia's article is also informative: https://en.wikipedia.org/wiki/Data_dredging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>By running many tests or experiments and then focusing only on the ones that worked, you can present false positives as real results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Keep in mind that this also applies to running many different models or different types of experiments and on different data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, out-of-sample testing is one of the best ways to reduce your risk. You should always use it, no matter the circumstances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often one of the ways that false positives make it through your workflow is a lack of an out-of-sample test at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources\n",
    " -  https://en.wikipedia.org/wiki/Multiple_comparisons_problem\n",
    " -  https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
    " -  https://en.wikipedia.org/wiki/Bonferroni_correction\n",
    " -  https://fivethirtyeight.com/features/science-isnt-broken/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
