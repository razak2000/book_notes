{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is <font color='blue'>a classical method for dimension reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>It uses the first several **principal components**, statistical features that explain most of the variation of a $m \\times n$ data matrix $\\mathbf{X}$, to describe the large-scale data matrix $\\mathbf{X}$ economically.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will introduce PCA with an image processing example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_image(m,n):\n",
    "    X = np.zeros((m,n))\n",
    "# generate a rectangle\n",
    "    X[25:80,25:80] = 1\n",
    "# generate a triangle\n",
    "    for i in range(25, 80, 1):\n",
    "        X[i+80:160, 100+i-1] = 2\n",
    "# generate a circle\n",
    "    for i in range(0,200,1):\n",
    "        for j in range(0,200,1):\n",
    "            if ((i - 135)*(i - 135) +(j - 53)*(j - 53) <= 900):\n",
    "                X[i, j] = 3\n",
    "    return X\n",
    "X = generate_test_image(200,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgplot = plt.imshow(X, cmap='gray')\n",
    "plt.title('Original Test Image');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X.shape[0] # num of rows\n",
    "n = X.shape[1] # num of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set each row as a variable, with observations in the columns. Denote the covariance matrix of $\\mathbf{X}$ as $\\mathbf{C}$, where the size of $\\mathbf{C}$ is $m \\times m$. $\\mathbf{C}$ is a matrix whose $(i,j)^{th}$ entry is the covariance between the $i^{th}$ row and $j^{th}$ row of the matrix $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(X, dtype=np.float64)\n",
    "C = np.cov(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Performing principal component analysis decomposes the matrix $\\mathbf{C}$ into:\n",
    "$\\mathbf{C} = \\mathbf{L}\\mathbf{P}\\mathbf{L}^{\\top},$ <br>  where $\\mathbf{P}$ is a diagonal matrix $\\mathbf{P}=\\text{diag}(\\lambda_1,\\lambda_2,\\dots,\\lambda_m)$, with $\\lambda_1 \\geq \\lambda_1 \\geq \\dots \\lambda_m \\geq 0$ being the eigenvalues of matrix $\\mathbf{C}$. \n",
    "\n",
    "<font color='blue'>The matrix $\\mathbf{L}$ is an orthogonal matrix, consisting the eigenvectors of matrix $\\mathbf{C}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, L = LA.eigh(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `LA.eigh` lists the eigenvalues from small to large in $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = P[::-1]\n",
    "L = L[:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(L.dot(np.diag(P)).dot(L.T), C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(P, '-o')\n",
    "plt.xlim([1, P.shape[0]])\n",
    "plt.xlabel('eigenvalue index')\n",
    "plt.ylabel('eigenvalue in a log scale')\n",
    "plt.title('Eigenvalues of Covariance Matrix');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>The $i^{th}$ **principal component** is given as $i^{th}$ row of $\\mathbf{V}$, \n",
    "$\\mathbf{V} =\\mathbf{L}^{\\top} \\mathbf{X}.$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = L.T.dot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>To approximate $\\mathbf{X}$, we use $k$ eigenvectors that have largest eigenvalues:\n",
    "$\\mathbf{X} \\approx \\mathbf{L[:, 1:k]}\\mathbf{L[:, 1:k]}^{\\top} \\mathbf{X}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote the approximated $\\mathbf{X}$ as $\\tilde{\\mathbf{X}} = \\mathbf{L[:, 1:k]}\\mathbf{L[:, 1:k]}^{\\top} \\mathbf{X}$. <font color='blue'>When $k = m $, the $\\tilde{\\mathbf{X}}$ should be same as $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 200\n",
    "X_tilde =  L[:,0:k-1].dot(L[:,0:k-1].T).dot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(X_tilde, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_tilde, cmap='gray')\n",
    "plt.title('Approximated Image with full rank');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>The proportion of total variance due to the $i^{th}$ principal component is given by the ratio $\\frac{\\lambda_i}{\\lambda_1 + \\lambda_2 + \\dots \\lambda_m}.$ \n",
    "\n",
    "The sum of proportion of total variance should be $1$. \n",
    "\n",
    "<font color='blue'>As we defined, $\\lambda_i$ is $i^{th}$ entry of $\\mathbf{P}$, $\\sum_{i}\\frac{P_i}{\\text{trace}(P)} = 1$\n",
    ", where the trace$(P)$ is the sum of the diagonal of $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(P/P.sum()).sum()b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((P/P.sum()).cumsum(), '-o')\n",
    "plt.title('Cumulative Sum of the Proportion of Total Variance')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('Proportion');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tilde_10 = L[:,0:10-1].dot(V[0:10-1,:])\n",
    "X_tilde_20 = L[:,0:20-1].dot(V[0:20-1,:])\n",
    "X_tilde_30 = L[:,0:30-1].dot(V[0:30-1,:])\n",
    "X_tilde_60 = L[:,0:60-1].dot(V[0:60-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 12))\n",
    "ax1.imshow(X_tilde_10, cmap='gray')\n",
    "ax1.set(title='Approximated Image with k = 10')\n",
    "ax2.imshow(X_tilde_20, cmap='gray')\n",
    "ax2.set(title='Approximated Image with k = 20')\n",
    "ax3.imshow(X_tilde_30, cmap='gray')\n",
    "ax3.set(title='Approximated Image with k = 30')\n",
    "ax4.imshow(X_tilde_60, cmap='gray')\n",
    "ax4.set(title='Approximated Image with k = 60');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving forward, we do not have to do PCA by hand. Luckly, [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) has an implementation that we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = ['IBM','MSFT', 'FB', 'T', 'INTC', 'ABX','NEM', 'AU', 'AEM', 'GFI']\n",
    "\n",
    "start = \"2015-09-01\"\n",
    "end = \"2016-11-01\"\n",
    "\n",
    "portfolio_returns = get_pricing(symbol, start_date=start, end_date=end, fields=\"price\").pct_change()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "num_pc = 2\n",
    "\n",
    "X = np.asarray(portfolio_returns)\n",
    "[n,m] = X.shape\n",
    "print 'The number of timestamps is {}.'.format(n)\n",
    "print 'The number of stocks is {}.'.format(m)\n",
    "\n",
    "pca = PCA(n_components=num_pc) # number of principal components\n",
    "pca.fit(X)\n",
    "\n",
    "percentage =  pca.explained_variance_ratio_\n",
    "percentage_cum = np.cumsum(percentage)\n",
    "print '{0:.2f}% of the variance is explained by the first 2 PCs'.format(percentage_cum[-1]*100)\n",
    "\n",
    "pca_components = pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1,len(percentage)+1,1)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(x, percentage*100, align = \"center\")\n",
    "plt.title('Contribution of principal components',fontsize = 16)\n",
    "plt.xlabel('principal components',fontsize = 16)\n",
    "plt.ylabel('percentage',fontsize = 16)\n",
    "plt.xticks(x,fontsize = 16) \n",
    "plt.yticks(fontsize = 16)\n",
    "plt.xlim([0, num_pc+1])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, percentage_cum*100,'ro-')\n",
    "plt.xlabel('principal components',fontsize = 16)\n",
    "plt.ylabel('percentage',fontsize = 16)\n",
    "plt.title('Cumulative contribution of principal components',fontsize = 16)\n",
    "plt.xticks(x,fontsize = 16) \n",
    "plt.yticks(fontsize = 16)\n",
    "plt.xlim([1, num_pc])\n",
    "plt.ylim([50,100]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>From these principal components we can construct \"statistical risk factors\", similar to more conventional common risk factors. These should give us an idea of how much of the portfolio's returns comes from some unobservable statistical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_returns = X.dot(pca_components.T)\n",
    "factor_returns = pd.DataFrame(columns=[\"factor 1\", \"factor 2\"], \n",
    "                              index=portfolio_returns.index,\n",
    "                              data=factor_returns)\n",
    "factor_returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_exposures = pd.DataFrame(index=[\"factor 1\", \"factor 2\"], \n",
    "                                columns=portfolio_returns.columns,\n",
    "                                data = pca.components_).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_exposures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = factor_exposures.index\n",
    "data = factor_exposures.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots_adjust(bottom = 0.1)\n",
    "plt.scatter(\n",
    "    data[:, 0], data[:, 1], marker='o', s=300, c='m',\n",
    "    cmap=plt.get_cmap('Spectral'))\n",
    "plt.title('Scatter Plot of Coefficients of PC1 and PC2')\n",
    "plt.xlabel('factor exposure of PC1')\n",
    "plt.ylabel('factor exposure of PC2')\n",
    "\n",
    "for label, x, y in zip(labels, data[:, 0], data[:, 1]):\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        xy=(x, y), xytext=(-20, 20),\n",
    "        textcoords='offset points', ha='right', va='bottom',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "        arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0')\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Creating statistical risk factors allows us to further break down the returns of a portfolio to get a better idea of the risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be used as an additional step after performance attribution with more common risk factors, such as those in the [Quantopian Risk Model](https://www.quantopian.com/risk-model), to try to account for additional unknown risks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>References:</b>\n",
    "- Datta, B.N., 2010. *Numerical linear algebra and applications*. Siam.\n",
    "- Qian, E.E., Hua, R.H. and Sorensen, E.H., 2007. *Quantitative equity portfolio management: modern techniques and applications*. CRC Press."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
