{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Value at Risk (VaR)</font> is a key concept in portfolio risk management. <font color='blue'>It uses the past observed distribution of portfolio returns to estimate what your future losses might be at different likelihood levels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use normal distributions to simulate the returns, in practice real returns will almost never follow normal distributions and usually have weird behavior including fat tails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu = 0.01, std = 0.10, 1000 bars, 10 assets\n",
    "mu = 0.01\n",
    "sigma = 0.10\n",
    "bars = 1000\n",
    "num_assets = 10\n",
    "\n",
    "returns = np.random.normal(mu, sigma, (bars, num_assets))\n",
    "\n",
    "# Fake asset names\n",
    "names = ['Asset %s' %i  for i in range(num_assets)]\n",
    "\n",
    "# Put in a pandas dataframe\n",
    "returns = pd.DataFrame(returns, columns=names)\n",
    "\n",
    "# Plot the last 50 bars\n",
    "plt.plot(returns.head(50))\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Return');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>The Value at Risk (VaR) for coverage $\\alpha$ is defined as the maximum amount we could expect to lose with likelihood $p = 1 - \\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Put another way, on no more that $100 \\times p \\%$ of days should we expect to lose more than the VaR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact <font color='blue'>you should not put complete trust in VaR, it is rather intended as a way to get a sense of how much might be lost in different levels of extreme scenarios, and provide this info to people responsible for risk management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>VaR for a high $\\alpha$ is a measure of worst case outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Because real distributions tend to diverge and become less and less consistent the further along the tail we go, extreme VaR should be taken with a grain of salt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those familiar with confidence intervals, VaR is very similar. The idea of trying to cover a set of possible values with an interval specified by $\\alpha$ is similar to how VaR tries to cover a set of possible losses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use <font color='blue'>historical VaR, which looks at previous returns distributions and uses that to compute the $p$ percentile. This percentile is the amount of loss you could reasonably expect to experience with probability $p$, assuming future returns are close to past returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this isn't perfect, and <font color='blue'>requires that there is no regime change in which the returns distribution changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, <font color='blue'>if your historical window doesn't include any crisis events, your VaR estimate will be far lower than it should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute historical VaR for coverage $\\alpha$ we simply take the $100 \\times (1 - \\alpha)$ percentile of lowest oberserved returns and multiply that by our total value invested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.ones((10, 1))\n",
    "# Normalize\n",
    "weights = weights / np.sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_at_risk(value_invested, returns, weights, alpha=0.95, lookback_days=520):\n",
    "    returns = returns.fillna(0.0)\n",
    "    # Multiply asset returns by weights to get one weighted portfolio return\n",
    "    portfolio_returns = returns.iloc[-lookback_days:].dot(weights)\n",
    "    # Compute the correct percentile loss and multiply by value invested\n",
    "    return np.percentile(portfolio_returns, 100 * (1-alpha)) * value_invested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_invested = 1000000\n",
    "\n",
    "value_at_risk(value_invested, returns, weights, alpha=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>A special case of VaR is when you assume that the returns follow a given distribution rather than non-parametrically estiamting it historically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case a <font color='blue'>normal VaR would fit our data, because all our returns were simulated form a normal distribution.</font> We can check this by using a normal distribution Cumulative Distribution Function (CDF), which sums the area under a normal curve to figure out how likely certain values are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use an <font color='blue'>inverse CDF, or PPF,</font> which for a given likelihood will tell us to which value that likelihood corresponds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Specifically, the closed form formula for Normal VaR is\n",
    "$VaR_{\\alpha}(x) = \\mu - \\sigma N^{-1}(\\alpha)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio mean return is unchanged, but std has to be recomputed\n",
    "# This is because independent variances sum, but std is sqrt of variance\n",
    "portfolio_std = np.sqrt( np.power(sigma, 2) * num_assets ) / num_assets\n",
    "\n",
    "# manually \n",
    "(mu - portfolio_std * norm.ppf(0.95)) * value_invested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the <font color='blue'>VaR is expressed as a return rather than an absolute loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_at_risk_N(mu=0, sigma=1.0, alpha=0.95):\n",
    "    return mu - sigma*norm.ppf(alpha)\n",
    "\n",
    "\n",
    "x = np.linspace(-3*sigma,3*sigma,1000)\n",
    "y = norm.pdf(x, loc=mu, scale=portfolio_std)\n",
    "plt.plot(x,y);\n",
    "plt.axvline(value_at_risk_N(mu = 0.01, sigma = portfolio_std, alpha=0.95), color='red', linestyle='solid');\n",
    "plt.legend(['Return Distribution', 'VaR for Specified Alpha as a Return'])\n",
    "plt.title('VaR in Closed Form for a Normal Distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Historical VaR instead uses historical data to draw a discrete Probability Density Function, or histogram. Then finds the point at which only $100 \\times (1-\\alpha)\\%$ of the points are below that return. It returns that return as the VaR return for coverage $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback_days = 520\n",
    "alpha = 0.95\n",
    "\n",
    "# Multiply asset returns by weights to get one weighted portfolio return\n",
    "portfolio_returns = returns.fillna(0.0).iloc[-lookback_days:].dot(weights)\n",
    "\n",
    "portfolio_VaR = value_at_risk(value_invested, returns, weights, alpha=0.95)\n",
    "# Need to express it as a return rather than absolute loss\n",
    "portfolio_VaR_return = portfolio_VaR / value_invested\n",
    "\n",
    "plt.hist(portfolio_returns, bins=20)\n",
    "plt.axvline(portfolio_VaR_return, color='red', linestyle='solid');\n",
    "plt.legend(['VaR for Specified Alpha as a Return', 'Historical Returns Distribution'])\n",
    "plt.title('Historical VaR');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>In real financial data the underlying distributions are rarely normal. This is why we prefer historical VaR as opposed to an assumption of an underlying distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Historical VaR is also non-parametric, so we aren't at risk of overfitting distribution parameters to some data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OEX components as of 3/31/16\n",
    "# http://www.cboe.com/products/indexcomponents.aspx?DIR=OPIndexComp&FILE=snp100.doc\n",
    "oex = ['MMM','T','ABBV','ABT','ACN','ALL','GOOGL','GOOG','MO','AMZN','AXP','AIG','AMGN','AAPL','BAC',\n",
    "       'BRK-B','BIIB','BLK','BA','BMY','CVS','COF','CAT','CELG','CVX','CSCO','C','KO','CL','CMCSA',\n",
    "       'COP','CSOT','DHR','DOW','DUK','DD','EMC','EMR','EXC','XOM','FB','FDX','F','GD','GE','GM','GILD',\n",
    "       'GS','HAL','HD','HON','INTC','IBM','JPM','JNJ','KMI','LLY','LMT','LOW','MA','MCD','MDT','MRK',\n",
    "       'MET,','MSFT','MDZL','MON','MS','NKE','NEE','OXY','ORCL','PYPL','PEP','PFE','PM','PG','QCOM',\n",
    "       'RTN','SLB','SPG','SO','SBUX','TGT','TXN','BK','PCLN','TWX','FOXA','FOX','USB','UNP','UPS','UTX',\n",
    "       'UNH','VZ','V','WMT','WBA','DIS','WFC']\n",
    "tickers = symbols(oex)\n",
    "num_stocks = len(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "data = get_pricing(tickers, fields='close_price', start_date='2014-01-01', end_date='2016-04-04')\n",
    "end = time.time()\n",
    "print \"Time: %0.2f seconds.\" % (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = data.pct_change()\n",
    "returns = returns - returns.mean(skipna=True) # de-mean the returns\n",
    "\n",
    "data.plot(legend=None);\n",
    "returns.plot(legend=None); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(x):\n",
    "    return x / np.sum(np.abs(x))\n",
    "\n",
    "weights = scale(np.random.random(num_stocks))\n",
    "plt.bar(np.arange(num_stocks),weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_at_risk(value_invested, returns, weights, alpha=0.95, lookback_days=520)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback_days = 520\n",
    "alpha = 0.95\n",
    "\n",
    "# Multiply asset returns by weights to get one weighted portfolio return\n",
    "portfolio_returns = returns.fillna(0.0).iloc[-lookback_days:].dot(weights)\n",
    "\n",
    "portfolio_VaR = value_at_risk(value_invested, returns, weights, alpha=0.95)\n",
    "# Need to express it as a return rather than absolute loss\n",
    "portfolio_VaR_return = portfolio_VaR / value_invested\n",
    "\n",
    "plt.hist(portfolio_returns, bins=20)\n",
    "plt.axvline(portfolio_VaR_return, color='red', linestyle='solid');\n",
    "plt.legend(['VaR for Specified Alpha as a Return', 'Historical Returns Distribution'])\n",
    "plt.title('Historical VaR');\n",
    "plt.xlabel('Return');\n",
    "plt.ylabel('Observation Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.stattools import jarque_bera\n",
    "\n",
    "_, pvalue, _, _ = jarque_bera(portfolio_returns)\n",
    "\n",
    "if pvalue > 0.05:\n",
    "    print 'The portfolio returns are likely normal.'\n",
    "else:\n",
    "    print 'The portfolio returns are likely not normal.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice <font color='blue'>the VaR computation conspicuously uses a lookback window. This is a parameter to the otherwise 'non-parametric' historical VaR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that because lookback window affects VaR, <font color='blue'>it's important to pick a lookback window that's long enough for the VaR to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Also keep in mind that even if something has converged on a say 500 day window, that may be ignoring a financial collapse that happened 1000 days ago, and therefore is ignoring crucial data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>On the other hand, using all time data may be useless for reasons of non-stationarity in returns varaince. Basically as returns variance changes over time, older measurements may reflect state that is no longer accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "VaRs = np.zeros((N, 1))\n",
    "for i in range(N):\n",
    "    VaRs[i] = value_at_risk(value_invested, returns, weights, lookback_days=i)\n",
    "\n",
    "plt.plot(VaRs)\n",
    "plt.xlabel('Lookback Window')\n",
    "plt.ylabel('VaR');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another check we'll do is for stationarity of the portfolio returns over this time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "results = adfuller(portfolio_returns)\n",
    "pvalue = results[1]\n",
    "\n",
    "if pvalue < 0.05:\n",
    "    print 'Process is likely stationary.'\n",
    "else:\n",
    "    print 'Process is likely non-stationary.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>CVaR </font>is what many consider an improvement on VaR, as it <font color='blue'>takes into account the shape of the returns distribution.\n",
    "\n",
    "<font color='blue'>It is also known as Expected Shortfall (ES), as it is an expectation over all the different possible losses greater than VaR and their corresponding estimated likelihoods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the probabilities are unequal and when the outcomes are continuous we have to use integration in closed form equations.\n",
    "\n",
    "Here is the formula for CVaR. \n",
    "<font color='blue'>$CVaR_{\\alpha}(x) \\approx \\frac{1}{(1-\\alpha)} \\int_{f(x,y) \\geq VaR_{\\alpha}(x)} f(x,y)p(y)dy dx$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvar(value_invested, returns, weights, alpha=0.95, lookback_days=520):\n",
    "    # Call out to our existing function\n",
    "    var = value_at_risk(value_invested, returns, weights, alpha, lookback_days=lookback_days)\n",
    "    returns = returns.fillna(0.0)\n",
    "    portfolio_returns = returns.iloc[-lookback_days:].dot(weights)\n",
    "    \n",
    "    # Get back to a return rather than an absolute loss\n",
    "    var_pct_loss = var / value_invested\n",
    "    \n",
    "    return value_invested * np.nanmean(portfolio_returns[portfolio_returns < var_pct_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvar(value_invested, returns, weights, lookback_days=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_at_risk(value_invested, returns, weights, lookback_days=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general <font color='blue'>it is considered to be a far superior metric compared with VaR and you should use it over VaR in most cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback_days = 520\n",
    "alpha = 0.95\n",
    "\n",
    "# Multiply asset returns by weights to get one weighted portfolio return\n",
    "portfolio_returns = returns.fillna(0.0).iloc[-lookback_days:].dot(weights)\n",
    "\n",
    "portfolio_VaR = value_at_risk(value_invested, returns, weights, alpha=0.95)\n",
    "# Need to express it as a return rather than absolute loss\n",
    "portfolio_VaR_return = portfolio_VaR / value_invested\n",
    "\n",
    "portfolio_CVaR = cvar(value_invested, returns, weights, alpha=0.95)\n",
    "# Need to express it as a return rather than absolute loss\n",
    "portfolio_CVaR_return = portfolio_CVaR / value_invested\n",
    "\n",
    "# Plot only the observations > VaR on the main histogram so the plot comes out\n",
    "# nicely and doesn't overlap.\n",
    "plt.hist(portfolio_returns[portfolio_returns > portfolio_VaR_return], bins=20)\n",
    "plt.hist(portfolio_returns[portfolio_returns < portfolio_VaR_return], bins=10)\n",
    "plt.axvline(portfolio_VaR_return, color='red', linestyle='solid');\n",
    "plt.axvline(portfolio_CVaR_return, color='red', linestyle='dashed');\n",
    "plt.legend(['VaR for Specified Alpha as a Return',\n",
    "            'CVaR for Specified Alpha as a Return',\n",
    "            'Historical Returns Distribution', \n",
    "            'Returns < VaR'])\n",
    "plt.title('Historical VaR and CVaR');\n",
    "plt.xlabel('Return');\n",
    "plt.ylabel('Observation Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "CVaRs = np.zeros((N, 1))\n",
    "for i in range(N):\n",
    "    CVaRs[i] = cvar(value_invested, returns, weights, lookback_days=i)\n",
    "\n",
    "plt.plot(CVaRs)\n",
    "plt.xlabel('Lookback Window')\n",
    "plt.ylabel('VaR');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <b>Sources</b>\n",
    " * http://www.wiley.com/WileyCDA/WileyTitle/productCd-1118445597.html\n",
    " * http://www.ise.ufl.edu/uryasev/publications/\n",
    " * http://www.ise.ufl.edu/uryasev/files/2011/11/VaR_vs_CVaR_CARISMA_conference_2010.pdf\n",
    " * http://faculty.washington.edu/ezivot/econ589/me20-1-4.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
