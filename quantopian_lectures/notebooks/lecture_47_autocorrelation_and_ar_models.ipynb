{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>An autoregressive, or AR$(p)$, model is created by regressing a time series on its past values, its lags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The simplest form of an autoregressive model is an <font color='blue'>AR$(1)$ model, signifying using only one lag term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first order autocorrelation model like this for a time series $x_t$ is:\n",
    "<font color='blue'>$x_t = b_0 + b_1 x_{t - 1} + \\epsilon_t$ </font><br>\n",
    "    where $x_{t - 1}$ represents the value of the time series at time $(t - 1)$ and $\\epsilon_t$ is the error term. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend this to an <font color='blue'>AR$(p)$ model, denoted:\n",
    "$x_t = b_0 + b_1 x_{t-1} + b_2 x_{t - 2} \\ldots + b_p x_{t - p} + \\epsilon_t$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>For an AR model to function properly, we must require that the time series is covariance stationary. \n",
    "\n",
    "This means that it follows three conditions:\n",
    "\n",
    "1. The <font color='blue'>expected value of the time series is constant and finite at all times, i.e. $E[y_t] = \\mu$ and $\\mu < \\infty$ for all values of $t$.\n",
    "2. The <font color='blue'>variance of the time series is constant and finite for all time periods.\n",
    "3. The <font color='blue'>covariance of the time series with itself for a fixed number of periods in either the future or the past is constant and finite for all time periods, i.e\n",
    "$\n",
    "COV(y_t, y_{t - s}) = \\lambda, \\  |\\lambda| < \\infty, \\text{ $\\lambda$ constant}, \\  t = 1, 2, \\ \\ldots, T; \\  s = 0, \\pm 1, \\pm 2, \\ldots, \\pm T\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If these conditions are not satisfied, our estimation results will not have real-world meaning. Our estimates for the parameters will be biased, making any tests that we try to form using the model invalid. \n",
    "\n",
    "<font color='blue'>Unfortunately, it can be a real pain to find a covariance-stationary time series in the wild in financial markets. \n",
    "\n",
    "<font color='blue'>There are ways, however to make a non-stationary time series stationary.</font> Once we have performed this transformation, we can build an autoregressive models under the above assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa as tsa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ensures experiment runs the same every time\n",
    "np.random.seed(100)\n",
    "\n",
    "# This function simluates an AR process, generating a new value based on historial values,\n",
    "# autoregressive coefficients b1 ... bk, and some randomness.\n",
    "def AR(b, X, mu, sigma):\n",
    "    l = min(len(b)-1, len(X))\n",
    "    b0 = b[0]\n",
    "    \n",
    "    return b0 + np.dot(b[1:l+1], X[-l:]) + np.random.normal(mu, sigma)\n",
    "\n",
    "b = np.array([0, 0.8, 0.1, 0.05])\n",
    "X = np.array([1])\n",
    "\n",
    "mu = 0\n",
    "sigma = 1\n",
    "\n",
    "for i in range(10000):\n",
    "    X = np.append(X, AR(b, X, mu, sigma))\n",
    "    \n",
    "plt.plot(X)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('AR Series Value');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Autoregressive processes will tend to have more extreme values than data drawn from say a normal distribution.</font> This is because the value at each time point is influenced by recent values. \n",
    "\n",
    "<font color='blue'>If the series randomly jumps up, it is more likely to stay up than a non-autoregressive series. This is known as 'fat-tailledness' (fat-tailed distribution) because the extremes on the pdf will be fatter than in a normal distribution.\n",
    "\n",
    "Much talk of tail risk in finance comes from the fact that tail events do occur and are hard to model due to their infrequent occurrence. \n",
    "\n",
    "<font color='blue'>If we have reason to suspect that a process is autoregressive, we should expect risk from extreme tail events and adjust accordingly.\n",
    "\n",
    "<font color='blue'>AR models are just one of the sources of tail risk, so don't assume that because a series is non-AR, it does not have tail risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tails_to_normal(X):\n",
    "    # Define matrix to store comparisons\n",
    "    A = np.zeros((2,4))    \n",
    "    for k in range(4):             \n",
    "        #stores tail probabilities of the sample series vs a normal series\n",
    "        A[0, k] = len(X[X > (k + 1)]) / float(len(X)) # Estimate tails of X        \n",
    "        A[1, k] = 1 - stats.norm.cdf(k + 1) # Compare to Gaussian distribution\n",
    "    print 'Frequency of std events in X \\n1: %s\\t2: %s\\t3: %s\\t4: %s' % tuple(A[0])\n",
    "    print 'Frequency of std events in a normal process \\n1: %s\\t2: %s\\t3: %s\\t4: %s' % tuple(A[1])\n",
    "    return A\n",
    "\n",
    "compare_tails_to_normal(X);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Because an AR process has a tail heavy and non-normal distribution of outcomes, <font color='blue'>estimates of variance on AR processes will be wrong. \n",
    "\n",
    "This is dangerous because variance is used to calculate many quantities in staistics, most importantly confidence intervals and p-values. \n",
    "\n",
    "<font color='blue'>Because the width of the confidence interval is often based on a variance estimate, we can no longer trust p-values that come from AR processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_unadjusted_interval(X):\n",
    "    T = len(X)\n",
    "    # Compute mu and sigma MLE\n",
    "    mu = np.mean(X)\n",
    "    sigma = np.std(X)\n",
    "    # Compute the bounds using standard error\n",
    "    lower = mu - 1.96 * (sigma/np.sqrt(T))\n",
    "    upper = mu + 1.96 * (sigma/np.sqrt(T))\n",
    "    return lower, upper\n",
    "\n",
    "# We'll make a function that returns true when the computed bounds contain 0\n",
    "def check_unadjusted_coverage(X):\n",
    "    l, u = compute_unadjusted_interval(X)\n",
    "    # Check to make sure l <= 0 <= u\n",
    "    if l <= 0 and u >= 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def simululate_AR_process(b, T):\n",
    "    X = np.array([1])\n",
    "\n",
    "    mu = 0\n",
    "    sigma = 1\n",
    "\n",
    "    for i in range(T):\n",
    "        X = np.append(X, AR(b, X, mu, sigma))\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 1000\n",
    "outcomes = np.zeros((trials, 1))\n",
    "\n",
    "for i in range(trials):\n",
    "    #note these are the same values we used to generate the initial AR array\n",
    "    Z = simululate_AR_process(np.array([0, 0.8, 0.1, 0.05]), 100)\n",
    "    if check_unadjusted_coverage(Z):\n",
    "        # The internal contains 0, the true value\n",
    "        outcomes[i] = 1\n",
    "    else:\n",
    "        outcomes[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(outcomes) / trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Stationarity tests should usually catch AR behavior and let us know that estimates of variance will be wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>In practice it can be very difficult to accurately estimate variance on an AR series, but one attempt to do this is the Newey-West estimation. You can find information on it [here](https://en.wikipedia.org/wiki/Newey%E2%80%93West_estimator)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>In order to determine the order, $p$, of an AR$(p)$ model, we look at the autocorrelations of the time series. </font>These are the correlations of the series with its past values. \n",
    "\n",
    "<font color='blue'>The $k$-th order autocorrelation is\n",
    "$\n",
    "\\rho_k = \\frac{COV(x_t, x_{t - k})}{\\sigma_x^2} = \\frac{E[(x_t - \\mu)(x_{t - k} - \\mu)}{\\sigma_x^2} </font>\n",
    "$.  Where $k$ represents the number of periods lagged. \n",
    "\n",
    "<font color='blue'>We cannot directly observe the autocorrelations so we estimate them as \n",
    "$\n",
    "\\hat{\\rho}_k = \\frac{\\sum_{t = k + 1}^T[(x_t - \\bar{x})(x_{t - k} - \\bar{x})]}{\\sum_{t = 1}^T (x_t - \\bar{x})^2}\n",
    "$\n",
    "\n",
    "For our purposes, we can use <font color='blue'>a pair of tools called the autocorrelation function (ACF) and the partial autocorrelation function (PACF) in order to determine the order of our model. The PACF controls for shorter lags, unlike the ACF. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf, pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = simululate_AR_process(np.array([0, 0.8, 0.1, 0.05]), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll choose 40 lags. This is a bit arbitrary, but you want to include all the lags you think might\n",
    "# feasibly impact the current value.\n",
    "nlags = 40\n",
    "# Note, this will produce nlags + 1 values, as we include the autocorrelation of\n",
    "# X[-1] with X[-1], which is trivially 1.\n",
    "# The reason this is done is because that is the 0th spot in the array and corresponds\n",
    "# to the 0th lag of X[(-1)-0].\n",
    "X_acf = acf(X, nlags=nlags)\n",
    "print 'Autocorrelations:\\n' + str(X_acf) + '\\n'\n",
    "X_pacf = pacf(X, nlags=nlags)\n",
    "print 'Partial Autocorrelations:\\n' + str(X_pacf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_acf, 'ro')\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.title(\"ACF\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_pacf, 'ro')\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.title(\"PACF\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>The `acf` and `pacf` functions will return confidence intervals on all the autocorrelations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to set a confidence level for our intervals, we choose the standard of 95%,\n",
    "# corresponding with an alpha of 0.05.\n",
    "X_acf, X_acf_confs = acf(X, nlags=nlags, alpha=0.05)\n",
    "X_pacf, X_pacf_confs = pacf(X, nlags=nlags, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acf(X_acf, X_acf_confs, title='ACF'):\n",
    "    # The confidence intervals are returned by the functions as (lower, upper)\n",
    "    # The plotting function needs them in the form (x-lower, upper-x)\n",
    "    errorbars = np.ndarray((2, len(X_acf)))\n",
    "    errorbars[0, :] = X_acf - X_acf_confs[:,0]\n",
    "    errorbars[1, :] = X_acf_confs[:,1] - X_acf\n",
    "\n",
    "    plt.plot(X_acf, 'ro')\n",
    "    plt.errorbar(range(len(X_acf)), X_acf, yerr=errorbars, fmt='none', ecolor='gray', capthick=2)\n",
    "    plt.xlabel('Lag')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.title(title);\n",
    "plot_acf(X_acf, X_acf_confs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(X_pacf, X_pacf_confs, title='PACF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>In a real-world time series, we use these plots to determine the order of our model. We would then attempt to fit a model using a maximum likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct an unfitted model\n",
    "model = tsa.api.AR(X)\n",
    "# Fit it\n",
    "model = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Parameters'\n",
    "print model.params\n",
    "print 'Standard Error'\n",
    "print model.bse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot this we'll need to format a confidence interval 2D array like the previous functions returned\n",
    "# Here is some quick code to do that\n",
    "model_confs = np.asarray((model.params - model.bse, model.params + model.bse)).T\n",
    "\n",
    "plot_acf(model.params, model_confs, title='Model Estimated Parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>The reason that AR models will estimate many more lags than is actually the case is due to indirect dependency.</font> If $X_t$ depends on $X_{t-1}$, then indirectly and to a lesser extent it will depend on $X_{t-2}$. \n",
    "\n",
    "<font color='blue'>In the presence of more than one lag in the data generating process, we will get potentially complex harmonic structures in the lags.</font> These indirect dependencies will be picked up by a simple estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general it's rarely the case that you can get anything useful out of a model with many parameters.\n",
    "\n",
    "In this case <font color='blue'>we want to select a number of lags that we believe explains what is happening, but without overfitting and choosing a model with way too many lags. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>We will use information criterion, specifically Akaike Information Criterion (AIC) and Bayes Information Criterion (BIC) to decide the correct number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the AIC and BIC is done as follows. <font color='blue'>Compute the AIC and BIC for all models we wish to consider, and note the smallest AIC and BIC recorded $AIC_{min}$ and $BIC_{min}$. \n",
    "\n",
    "These are the models which minimize information loss under each metric. \n",
    "\n",
    "<font color='blue'>For each type of IC We then can compute the *relative likelihood* of each model $i$ by taking \n",
    "$l = e^{(IC_{min} - IC_{i})/2}$\n",
    "\n",
    "<font color='blue'>We can interpret $l$ as model $i$ is $l$ times as likely to minimize information loss, compared to the minimum AIC model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "AIC = np.zeros((N, 1))\n",
    "\n",
    "for i in range(N):\n",
    "    model = tsa.api.AR(X)\n",
    "    model = model.fit(maxlag=(i+1))\n",
    "    AIC[i] = model.aic\n",
    "    \n",
    "AIC_min = np.min(AIC)\n",
    "model_min = np.argmin(AIC)\n",
    "\n",
    "print 'Relative Likelihoods'\n",
    "print np.exp((AIC_min-AIC) / 2)\n",
    "print 'Number of parameters in minimum AIC model %s' % (model_min+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "BIC = np.zeros((N, 1))\n",
    "\n",
    "for i in range(N):\n",
    "    model = tsa.api.AR(X)\n",
    "    model = model.fit(maxlag=(i+1))\n",
    "    BIC[i] = model.bic\n",
    "    \n",
    "BIC_min = np.min(BIC)\n",
    "model_min = np.argmin(BIC)\n",
    "\n",
    "print 'Relative Likelihoods'\n",
    "print np.exp((BIC_min-BIC) / 2)\n",
    "print 'Number of parameters in minimum BIC model %s' % (model_min+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't assume that using this method will always get you the right answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>One final step we might do before performing an out of sample test for this model would be to evaluate its residual behavior. \n",
    "\n",
    "The AIC and BIC already do this to an extent, effectively measuring how much information is left on the table (in the residuals) after the model has made its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tsa.api.AR(X)\n",
    "model = model.fit(maxlag=3)\n",
    "\n",
    "from statsmodels.stats.stattools import jarque_bera\n",
    "\n",
    "score, pvalue, _, _ = jarque_bera(model.resid)\n",
    "\n",
    "if pvalue < 0.10:\n",
    "    print 'We have reason to suspect the residuals are not normally distributed.'\n",
    "else:\n",
    "    print 'The residuals seem normally distributed.'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
