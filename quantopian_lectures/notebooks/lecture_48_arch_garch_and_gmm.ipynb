{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>AutoRegressive Conditionally Heteroskedastic (ARCH) occurs when the volatility of a time series is also autoregressive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxopt\n",
    "from functools import partial\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.stattools import jarque_bera\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by <font color='blue'> using Monte Carlo sampling to simulate a GARCH(1, 1) process.</font> Our dynamics will be\n",
    "\n",
    "<font color='blue'>$$\\sigma_1 = \\sqrt{\\frac{a_0}{1-a_1-b_1}} \\\\\n",
    "\\sigma_t^2 = a_0 + a_1 x_{t-1}^2+b_1 \\sigma_{t-1}^2 \\\\\n",
    "x_t = \\sigma_t \\epsilon_t \\\\\n",
    "\\epsilon \\sim \\mathcal{N}(0, 1)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "a0 = 1.0\n",
    "a1 = 0.1\n",
    "b1 = 0.8\n",
    "sigma1 = math.sqrt(a0 / (1 - a1 - b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_GARCH(T, a0, a1, b1, sigma1):\n",
    "    \n",
    "    # Initialize our values\n",
    "    X = np.ndarray(T)\n",
    "    sigma = np.ndarray(T)\n",
    "    sigma[0] = sigma1\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        # Draw the next x_t\n",
    "        X[t - 1] = sigma[t - 1] * np.random.normal(0, 1)\n",
    "        # Draw the next sigma_t\n",
    "        sigma[t] = math.sqrt(a0 + b1 * sigma[t - 1]**2 + a1 * X[t - 1]**2)\n",
    "        \n",
    "    X[T - 1] = sigma[T - 1] * np.random.normal(0, 1)    \n",
    "    \n",
    "    return X, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _ = simulate_GARCH(10000, a0, a1, b1, sigma1)\n",
    "X = X[1000:] # Drop burn in\n",
    "X = X / np.std(X) # Normalize X\n",
    "\n",
    "def compare_tails_to_normal(X):\n",
    "    # Define matrix to store comparisons\n",
    "    A = np.zeros((2,4))\n",
    "    for k in range(4):\n",
    "        A[0, k] = len(X[X > (k + 1)]) / float(len(X)) # Estimate tails of X\n",
    "        A[1, k] = 1 - stats.norm.cdf(k + 1) # Compare to Gaussian distribution\n",
    "    return A\n",
    "\n",
    "compare_tails_to_normal(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X, bins=50)\n",
    "plt.xlabel('sigma')\n",
    "plt.ylabel('observations');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample values from a normal distribution\n",
    "X2 = np.random.normal(0, 1, 9000)\n",
    "both = np.matrix([X, X2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot both the GARCH and normal values\n",
    "plt.plot(both.T, alpha=.7);\n",
    "plt.axhline(X2.std(), color='yellow', linestyle='--')\n",
    "plt.axhline(-X2.std(), color='yellow', linestyle='--')\n",
    "plt.axhline(3*X2.std(), color='red', linestyle='--')\n",
    "plt.axhline(-3*X2.std(), color='red', linestyle='--')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('sigma');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>The first step is to test for ARCH conditions. To do this we run a regression on $x_t$ fitting the following model.\n",
    "$x_t^2 = a_0 + a_1 x_{t-1}^2 + \\dots + a_p x_{t-p}^2$\n",
    "\n",
    "<font color='blue'>We use OLS to estimate $\\hat\\theta = (\\hat a_0, \\hat a_1, \\dots, \\hat a_p)$ and the covariance matrix $\\hat\\Omega$. We can then compute the test statistic\n",
    "$F = \\hat\\theta \\hat\\Omega^{-1} \\hat\\theta'$\n",
    "\n",
    "<font color='blue'>We will reject if $F$ is greater than the 95% confidence bars in the $\\mathcal(X)^2(p)$ distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _ = simulate_GARCH(1100, a0, a1, b1, sigma1)\n",
    "X = X[100:] # Drop burn in\n",
    "\n",
    "p = 20\n",
    "\n",
    "# Drop the first 20 so we have a lag of p's\n",
    "Y2 = (X**2)[p:]\n",
    "X2 = np.ndarray((980, p))\n",
    "for i in range(p, 1000):\n",
    "    X2[i - p, :] = np.asarray((X**2)[i-p:i])[::-1]\n",
    "\n",
    "model = sm.OLS(Y2, X2)\n",
    "model = model.fit()\n",
    "theta = np.matrix(model.params)\n",
    "omega = np.matrix(model.cov_HC0)\n",
    "F = np.asscalar(theta * np.linalg.inv(omega) * theta.T)\n",
    "\n",
    "print np.asarray(theta.T).shape\n",
    "\n",
    "plt.plot(range(20), np.asarray(theta.T))\n",
    "plt.xlabel('Lag Amount')\n",
    "plt.ylabel('Estimated Coefficient for Lagged Datapoint')\n",
    "\n",
    "print 'F = ' + str(F)\n",
    "\n",
    "chi2dist = scipy.stats.chi2(p)\n",
    "pvalue = 1-chi2dist.cdf(F)\n",
    "print 'p-value = ' + str(pvalue)\n",
    "\n",
    "# Finally let's look at the significance of each a_p as measured by the standard deviations away from 0\n",
    "print theta/np.diag(omega)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Once we've decided that the data might have an underlying GARCH(1, 1) model, we would like to fit GARCH(1, 1) to the data by estimating parameters.\n",
    "\n",
    "<font color='blue'>To do this we need the log-likelihood function\n",
    "$\\mathcal{L}(\\theta) = \\sum_{t=1}^T - \\ln \\sqrt{2\\pi} - \\frac{x_t^2}{2\\sigma_t^2} - \\frac{1}{2}\\ln(\\sigma_t^2)$\n",
    "\n",
    "To evaluate this function we need $x_t$ and $\\sigma_t$ for $1 \\leq t \\leq T$. We have $x_t$, but we need to compute $\\sigma_t$. To do this <font color='blue'>we need to make a guess for $\\sigma_1$. Our guess will be $\\sigma_1^2 = \\hat E[x_t^2]$. \n",
    "\n",
    "<font color='blue'>Once we have our initial guess we compute the rest of the $\\sigma$'s using the equation\n",
    "$\\sigma_t^2 = a_0 + a_1 x_{t-1}^2 + b_1\\sigma_{t-1}^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _ = simulate_GARCH(10000, a0, a1, b1, sigma1)\n",
    "X = X[1000:] # Drop burn in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's our function to compute the sigmas given the initial guess\n",
    "def compute_squared_sigmas(X, initial_sigma, theta):\n",
    "    \n",
    "    a0 = theta[0]\n",
    "    a1 = theta[1]\n",
    "    b1 = theta[2]\n",
    "    \n",
    "    T = len(X)\n",
    "    sigma2 = np.ndarray(T)\n",
    "    \n",
    "    sigma2[0] = initial_sigma ** 2\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        # Here's where we apply the equation\n",
    "        sigma2[t] = a0 + a1 * X[t-1]**2 + b1 * sigma2[t-1]\n",
    "    \n",
    "    return sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(X)), compute_squared_sigmas(X, np.sqrt(np.mean(X**2)), (1, 0.5, 0.5)))\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Sigma');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that we return the negative log likelihood, as this way our numerical optimizer can minimize the function while maximizing the log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood(X, theta):\n",
    "    \n",
    "    T = len(X)\n",
    "    \n",
    "    # Estimate initial sigma squared\n",
    "    initial_sigma = np.sqrt(np.mean(X ** 2))\n",
    "    \n",
    "    # Generate the squared sigma values\n",
    "    sigma2 = compute_squared_sigmas(X, initial_sigma, theta)\n",
    "    \n",
    "    # Now actually compute\n",
    "    return -sum(\n",
    "        [-np.log(np.sqrt(2.0 * np.pi)) -\n",
    "        (X[t] ** 2) / (2.0 * sigma2[t]) -\n",
    "        0.5 * np.log(sigma2[t]) for\n",
    "         t in range(T)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we <font color='blue'>perform numerical optimization to find our estimate for\n",
    "$\\hat\\theta = \\arg \\max_{(a_0, a_1, b_1)}\\mathcal{L}(\\theta) = \\arg \\min_{(a_0, a_1, b_1)}-\\mathcal{L}(\\theta)$\n",
    "\n",
    "We have some constraints on this\n",
    "<font color='blue'>$a_1 \\geq 0, b_1 \\geq 0, a_1+b_1 < 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make our objective function by plugging X into our log likelihood function\n",
    "objective = partial(negative_log_likelihood, X)\n",
    "\n",
    "# Define the constraints for our minimizer\n",
    "def constraint1(theta):\n",
    "    return np.array([1 - (theta[1] + theta[2])])\n",
    "\n",
    "def constraint2(theta):\n",
    "    return np.array([theta[1]])\n",
    "\n",
    "def constraint3(theta):\n",
    "    return np.array([theta[2]])\n",
    "\n",
    "cons = ({'type': 'ineq', 'fun': constraint1},\n",
    "        {'type': 'ineq', 'fun': constraint2},\n",
    "        {'type': 'ineq', 'fun': constraint3})\n",
    "\n",
    "# Actually do the minimization\n",
    "result = scipy.optimize.minimize(objective, (1, 0.5, 0.5),\n",
    "                        method='SLSQP',\n",
    "                        constraints = cons)\n",
    "theta_mle = result.x\n",
    "print 'theta MLE: ' + str(theta_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like a way to <font color='blue'>check our estimate</font>. We'll look at two things:\n",
    "1. <font color='blue'>How fat are the tails of the residuals.\n",
    "2. <font color='blue'>How normal are the residuals under the Jarque-Bera normality test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_theta_estimate(X, theta_estimate):\n",
    "    initial_sigma = np.sqrt(np.mean(X ** 2))\n",
    "    sigma = np.sqrt(compute_squared_sigmas(X, initial_sigma, theta_estimate))\n",
    "    epsilon = X / sigma\n",
    "    print 'Tails table'\n",
    "    print compare_tails_to_normal(epsilon / np.std(epsilon))\n",
    "    print ''\n",
    "    \n",
    "    _, pvalue, _, _ = jarque_bera(epsilon)\n",
    "    print 'Jarque-Bera probability normal: ' + str(pvalue)\n",
    "    \n",
    "check_theta_estimate(X, theta_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've just computed an estimate using MLE, but <font color='blue'>we can also use Generalized Method of Moments (GMM) to estimate the GARCH(1, 1) parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>To do this we need to define our moments. We'll use 4.\n",
    "1. The residual <font color='blue'>$\\hat\\epsilon_t = x_t / \\hat\\sigma_t$\n",
    "2. The variance of the residual <font color='blue'>$\\hat\\epsilon_t^2$\n",
    "3. The skew moment <font color='blue'>$\\mu_3/\\hat\\sigma_t^3 = (\\hat\\epsilon_t - E[\\hat\\epsilon_t])^3 / \\hat\\sigma_t^3$\n",
    "4. The kurtosis moment <font color='blue'>$\\mu_4/\\hat\\sigma_t^4 = (\\hat\\epsilon_t - E[\\hat\\epsilon_t])^4 / \\hat\\sigma_t^4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The n-th standardized moment\n",
    "# skewness is 3, kurtosis is 4\n",
    "def standardized_moment(x, mu, sigma, n):\n",
    "    return ((x - mu) ** n) / (sigma ** n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>GMM now has three steps.\n",
    "\n",
    "Start with <font color='blue'>$W$ as the identity matrix.\n",
    "1. <font color='blue'>Estimate $\\hat\\theta_1$ by using numerical optimization to minimize, \n",
    "$\\min_{\\theta \\in \\Theta} \\left(\\frac{1}{T} \\sum_{t=1}^T g(x_t, \\hat\\theta)\\right)' W \\left(\\frac{1}{T}\\sum_{t=1}^T g(x_t, \\hat\\theta)\\right)$\n",
    "2. <font color='blue'>Recompute $W$ based on the covariances of the estimated $\\theta$. (Focus more on parameters with explanatory power), \n",
    "$\\hat W_{i+1} = \\left(\\frac{1}{T}\\sum_{t=1}^T g(x_t, \\hat\\theta_i)g(x_t, \\hat\\theta_i)'\\right)^{-1}$\n",
    "3. <font color='blue'>Repeat until $|\\hat\\theta_{i+1} - \\hat\\theta_i| < \\epsilon$ or we reach an iteration threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_objective(X, W, theta):\n",
    "    # Compute the residuals for X and theta\n",
    "    initial_sigma = np.sqrt(np.mean(X ** 2))\n",
    "    sigma = np.sqrt(compute_squared_sigmas(X, initial_sigma, theta))\n",
    "    e = X / sigma\n",
    "    \n",
    "    # Compute the mean moments\n",
    "    m1 = np.mean(e)\n",
    "    m2 = np.mean(e ** 2) - 1\n",
    "    m3 = np.mean(standardized_moment(e, np.mean(e), np.std(e), 3))\n",
    "    m4 = np.mean(standardized_moment(e, np.mean(e), np.std(e), 4) - 3)\n",
    "    \n",
    "    G = np.matrix([m1, m2, m3, m4]).T\n",
    "    \n",
    "    return np.asscalar(G.T * W * G)\n",
    "\n",
    "def gmm_variance(X, theta):\n",
    "    # Compute the residuals for X and theta    \n",
    "    initial_sigma = np.sqrt(np.mean(X ** 2))\n",
    "    sigma = np.sqrt(compute_squared_sigmas(X, initial_sigma, theta))\n",
    "    e = X / sigma\n",
    "\n",
    "    # Compute the squared moments\n",
    "    m1 = e ** 2\n",
    "    m2 = (e ** 2 - 1) ** 2\n",
    "    m3 = standardized_moment(e, np.mean(e), np.std(e), 3) ** 2\n",
    "    m4 = (standardized_moment(e, np.mean(e), np.std(e), 4) - 3) ** 2\n",
    "    \n",
    "    # Compute the covariance matrix g * g'\n",
    "    T = len(X)\n",
    "    s = np.ndarray((4, 1))\n",
    "    for t in range(T):\n",
    "        G = np.matrix([m1[t], m2[t], m3[t], m4[t]]).T\n",
    "        s = s + G * G.T\n",
    "    \n",
    "    return s / T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GMM parameters\n",
    "W = np.identity(4)\n",
    "gmm_iterations = 10\n",
    "\n",
    "# First guess\n",
    "theta_gmm_estimate = theta_mle\n",
    "\n",
    "# Perform iterated GMM\n",
    "for i in range(gmm_iterations):\n",
    "    # Estimate new theta\n",
    "    objective = partial(gmm_objective, X, W)\n",
    "    result = scipy.optimize.minimize(objective, theta_gmm_estimate, constraints=cons)\n",
    "    theta_gmm_estimate = result.x\n",
    "    print 'Iteration ' + str(i) + ' theta: ' + str(theta_gmm_estimate)\n",
    "    \n",
    "    # Recompute W\n",
    "    W = np.linalg.inv(gmm_variance(X, theta_gmm_estimate))\n",
    "    \n",
    "\n",
    "check_theta_estimate(X, theta_gmm_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've fitted a model to our observations, we'd like to be able to <font color='blue'>predict what the future volatility will look like.\n",
    "\n",
    "To do this, we can just simulate more values using our original GARCH dynamics and the estimated parameters.\n",
    "\n",
    "The first thing we'll do is compute an initial $\\sigma_t$. We'll compute our squared sigmas and take the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_hats = np.sqrt(compute_squared_sigmas(X, np.sqrt(np.mean(X**2)), theta_mle))\n",
    "initial_sigma = sigma_hats[-1]\n",
    "initial_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0_estimate = theta_gmm_estimate[0]\n",
    "a1_estimate = theta_gmm_estimate[1]\n",
    "b1_estimate = theta_gmm_estimate[2]\n",
    "\n",
    "X_forecast, sigma_forecast = simulate_GARCH(100, a0_estimate, a1_estimate, b1_estimate, initial_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(-100, 0), X[-100:], 'b-')\n",
    "plt.plot(range(-100, 0), sigma_hats[-100:], 'r-')\n",
    "plt.plot(range(0, 100), X_forecast, 'b--')\n",
    "plt.plot(range(0, 100), sigma_forecast, 'r--')\n",
    "plt.xlabel('Time')\n",
    "plt.legend(['X', 'sigma']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>One should note that because we are moving foward using a random walk, this analysis is supposed to give us a sense of the magnitude of sigma and therefore the risk we could face. It is not supposed to accurately model future values of X. \n",
    "\n",
    "<font color='blue'>In practice you would probably want to use Monte Carlo sampling to generate thousands of future scenarios, and then look at the potential range of outputs.  \n",
    "\n",
    "Keep in mind that this is a fairly simplistic way of doing this analysis, and that <font color='blue'>better techniques, such as Bayesian cones, exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(-100, 0), X[-100:], 'b-')\n",
    "plt.plot(range(-100, 0), sigma_hats[-100:], 'r-')\n",
    "plt.xlabel('Time')\n",
    "plt.legend(['X', 'sigma'])\n",
    "\n",
    "\n",
    "max_X = [-np.inf]\n",
    "min_X = [np.inf]\n",
    "for i in range(100):\n",
    "    X_forecast, sigma_forecast = simulate_GARCH(100, a0_estimate, a1_estimate, b1_estimate, initial_sigma)\n",
    "    if max(X_forecast) > max(max_X):\n",
    "        max_X = X_forecast\n",
    "    elif min(X_forecast) < min(max_X):\n",
    "        min_X = X_forecast\n",
    "    plt.plot(range(0, 100), X_forecast, 'b--', alpha=0.05)\n",
    "    plt.plot(range(0, 100), sigma_forecast, 'r--', alpha=0.05)\n",
    "\n",
    "# Draw the most extreme X values specially\n",
    "plt.plot(range(0, 100), max_X, 'g--', alpha=1.0)\n",
    "plt.plot(range(0, 100), min_X, 'g--', alpha=1.0);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
